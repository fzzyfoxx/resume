{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b9f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.cfg.precompiled import get_llm, get_checkpointer\n",
    "from fcgb.tools.spectools import PhantomResearcherSpecTool, JobHandlerSpecTool, JobHandler\n",
    "from fcgb.cfg.chat_inputs_spec import JobHanlderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fab678",
   "metadata": {},
   "source": [
    "## Fake LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab4fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_llm = get_llm(llm_model='fake')\n",
    "memory = get_checkpointer(checkpointer_mode='local', mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66637a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_containers = [\n",
    "    PhantomResearcherSpecTool(llm=fake_llm, memory=memory)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfbceba",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_handler = JobHandlerSpecTool(\n",
    "    llm=fake_llm,\n",
    "    tool_containers=tool_containers,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce74a647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thread_id': '3e132091992a4d38af5ab16923d05857', 'output': 'Fake LLM response 2'}\n"
     ]
    }
   ],
   "source": [
    "outputs = job_handler.run(job_description='some job description')\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68c1d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_state = job_handler.get_state(thread_id=outputs['thread_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ce20af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_description': 'some job description',\n",
       " 'turns': 2,\n",
       " 'messages': [SystemMessage(content='Your purpose is to conduct a single job that will be provided to you by the user.\\nYou are also equipped with a set of tools that you can use to complete the job.\\n\\nPut attention on the job description as well as output format if specified.\\nYou have maximum of 4 turns to call the tools and complete the job.\\nYou can call the same tool multiple times (even within single turn) with different parameters to solve the job.\\nIf you will exceed the maximum number of turns, tools will be disabled and you will have to answer with the information you have at that moment.', additional_kwargs={}, response_metadata={}, id='350022a1-bcaa-4180-a285-ebbcc6d2540d'),\n",
       "  HumanMessage(content='Process the following job:\\n\\nsome job description\\n\\nBe aware of provided restrictions and output format.\\nStick to the job description and do not provide any additional information or context.\\nIf there is any additional data provided use it for your answer. You can also pass it directly to tools if they allows it.\\n\\nIn your final answer also write a title that summarizes the output.', additional_kwargs={}, response_metadata={}, id='d827bfd5-ca3e-4905-b297-6266a0c219e1'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={}, id='aeb0e0cd-ec99-4a47-b531-4b3643e3164a', tool_calls=[{'name': 'external_research', 'args': {'job': 'Fake string xcror', 'restrictions': 'Fake string fdppx', 'output_format': 'Fake string axtqd', 'data': 'Fake string rrsgl'}, 'id': 'call_8e9d08afcb004071b9fc34b038bb9679', 'type': 'tool_call'}, {'name': 'external_research', 'args': {'job': 'Fake string oidiq', 'restrictions': 'Fake string eiwkw', 'output_format': 'Fake string mkkgt', 'data': 'Fake string vuyjz'}, 'id': 'call_9bcc25d8c6154831be1770a03e13aa11', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='Fake LLM response 1', name='external_research', id='dfa2a2f1-83a1-4f4e-8cd8-636afc624f41', tool_call_id='call_8e9d08afcb004071b9fc34b038bb9679'),\n",
       "  ToolMessage(content='Fake LLM response 2', name='external_research', id='e51b2680-c4a6-45d3-8ca7-768c2a15d569', tool_call_id='call_9bcc25d8c6154831be1770a03e13aa11'),\n",
       "  AIMessage(content='Fake LLM response 2', additional_kwargs={}, response_metadata={}, id='6e5eff22-1112-4551-b981-7bce2402011e')],\n",
       " 'tools_threads': ['3ef7ac6888594237975f75f41140334f',\n",
       "  '68be493ec4354c1180d530ab913a47aa'],\n",
       " 'thread_id': '3e132091992a4d38af5ab16923d05857',\n",
       " 'output': 'Fake LLM response 2'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b59197d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Your purpose is to conduct a single job that will be provided to you by the user.\n",
      "You are also equipped with a set of tools that you can use to complete the job.\n",
      "\n",
      "Put attention on the job description as well as output format if specified.\n",
      "You have maximum of 4 turns to call the tools and complete the job.\n",
      "You can call the same tool multiple times (even within single turn) with different parameters to solve the job.\n",
      "If you will exceed the maximum number of turns, tools will be disabled and you will have to answer with the information you have at that moment.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Process the following job:\n",
      "\n",
      "some job description\n",
      "\n",
      "Be aware of provided restrictions and output format.\n",
      "Stick to the job description and do not provide any additional information or context.\n",
      "If there is any additional data provided use it for your answer. You can also pass it directly to tools if they allows it.\n",
      "\n",
      "In your final answer also write a title that summarizes the output.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  external_research (call_8e9d08afcb004071b9fc34b038bb9679)\n",
      " Call ID: call_8e9d08afcb004071b9fc34b038bb9679\n",
      "  Args:\n",
      "    job: Fake string xcror\n",
      "    restrictions: Fake string fdppx\n",
      "    output_format: Fake string axtqd\n",
      "    data: Fake string rrsgl\n",
      "  external_research (call_9bcc25d8c6154831be1770a03e13aa11)\n",
      " Call ID: call_9bcc25d8c6154831be1770a03e13aa11\n",
      "  Args:\n",
      "    job: Fake string oidiq\n",
      "    restrictions: Fake string eiwkw\n",
      "    output_format: Fake string mkkgt\n",
      "    data: Fake string vuyjz\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: external_research\n",
      "\n",
      "Fake LLM response 1\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: external_research\n",
      "\n",
      "Fake LLM response 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Fake LLM response 2\n"
     ]
    }
   ],
   "source": [
    "for msg in job_state['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38efa06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job': 'Fake string xcror', 'restrictions': 'Fake string fdppx', 'output_format': 'Fake string axtqd', 'data': 'Fake string rrsgl', 'messages': [SystemMessage(content='Your purpose is to mimic a research tool that uses external resources like web pages, research papers, wikipedia etc. to answer questions and provide information.\\nIf you will be asked to use those external resources use your best knowledge to provide the most accurate and relevant information whitout using any external resources but within your answer you can pretend that you are using them.\\n\\nYou are a part of a testing tool so accuracy is not that important but try to be as accurate as possible.\\n\\nAvoid any unnecessary comments that are not a integral part of the answer.\\n\\n', additional_kwargs={}, response_metadata={}, id='bbc613d3-6cfc-47a4-b3bb-12a7c611c064'), HumanMessage(content=\"Provide an answer for a research job which is:\\n**research job**: Fake string xcror\\n\\nWhen writing your answer be aware of provided restrictions:\\n**restrictions**: Fake string fdppx\\n\\nThere is also specified output format of your answer:\\n**output format**: Fake string axtqd\\nYour response should be structured according to the output format provided, ensuring clarity and organization in your answer.\\n\\nIn addition, you can use the following additional data, that the job may relate to, to support your answer:\\n**additional data**:\\nFake string rrsgl\\n\\n--------------------\\n\\nYour answer should be a detailed response that addresses the research job. \\nUse your best knowledge to provide relevant information, and if necessary, you can pretend to use external resources like web pages, research papers, or Wikipedia to support your answer. \\nRemember, accuracy is important, but since this is a testing tool, it doesn't have to be perfect. Focus on delivering a comprehensive and informative response that aligns with the research job's requirements.\\n\", additional_kwargs={}, response_metadata={}, id='a496ace4-e91a-4b0c-b87a-e542d4f9f30a'), AIMessage(content='Fake LLM response 1', additional_kwargs={}, response_metadata={}, id='d3b513fd-29a4-45c1-be92-0fe2d56ed540')], 'thread_id': '3ef7ac6888594237975f75f41140334f', 'output': 'Fake LLM response 1'}\n",
      "--------------------------------------------------\n",
      "{'job': 'Fake string oidiq', 'restrictions': 'Fake string eiwkw', 'output_format': 'Fake string mkkgt', 'data': 'Fake string vuyjz', 'messages': [SystemMessage(content='Your purpose is to mimic a research tool that uses external resources like web pages, research papers, wikipedia etc. to answer questions and provide information.\\nIf you will be asked to use those external resources use your best knowledge to provide the most accurate and relevant information whitout using any external resources but within your answer you can pretend that you are using them.\\n\\nYou are a part of a testing tool so accuracy is not that important but try to be as accurate as possible.\\n\\nAvoid any unnecessary comments that are not a integral part of the answer.\\n\\n', additional_kwargs={}, response_metadata={}, id='923debb3-f644-4082-b6bf-b93b1e079de6'), HumanMessage(content=\"Provide an answer for a research job which is:\\n**research job**: Fake string oidiq\\n\\nWhen writing your answer be aware of provided restrictions:\\n**restrictions**: Fake string eiwkw\\n\\nThere is also specified output format of your answer:\\n**output format**: Fake string mkkgt\\nYour response should be structured according to the output format provided, ensuring clarity and organization in your answer.\\n\\nIn addition, you can use the following additional data, that the job may relate to, to support your answer:\\n**additional data**:\\nFake string vuyjz\\n\\n--------------------\\n\\nYour answer should be a detailed response that addresses the research job. \\nUse your best knowledge to provide relevant information, and if necessary, you can pretend to use external resources like web pages, research papers, or Wikipedia to support your answer. \\nRemember, accuracy is important, but since this is a testing tool, it doesn't have to be perfect. Focus on delivering a comprehensive and informative response that aligns with the research job's requirements.\\n\", additional_kwargs={}, response_metadata={}, id='ae453abb-06df-4100-b0dd-f7ee07c52327'), AIMessage(content='Fake LLM response 2', additional_kwargs={}, response_metadata={}, id='615bf013-6e40-4c51-b2bc-4833657d043d')], 'thread_id': '68be493ec4354c1180d530ab913a47aa', 'output': 'Fake LLM response 2'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for tool_thread in job_state['tools_threads']:\n",
    "    tool_state = job_handler.get_state(thread_id=tool_thread)\n",
    "    print(tool_state)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa71fce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Your purpose is to mimic a research tool that uses external resources like web pages, research papers, wikipedia etc. to answer questions and provide information.\n",
      "If you will be asked to use those external resources use your best knowledge to provide the most accurate and relevant information whitout using any external resources but within your answer you can pretend that you are using them.\n",
      "\n",
      "You are a part of a testing tool so accuracy is not that important but try to be as accurate as possible.\n",
      "\n",
      "Avoid any unnecessary comments that are not a integral part of the answer.\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Provide an answer for a research job which is:\n",
      "**research job**: Fake string xcror\n",
      "\n",
      "When writing your answer be aware of provided restrictions:\n",
      "**restrictions**: Fake string fdppx\n",
      "\n",
      "There is also specified output format of your answer:\n",
      "**output format**: Fake string axtqd\n",
      "Your response should be structured according to the output format provided, ensuring clarity and organization in your answer.\n",
      "\n",
      "In addition, you can use the following additional data, that the job may relate to, to support your answer:\n",
      "**additional data**:\n",
      "Fake string rrsgl\n",
      "\n",
      "--------------------\n",
      "\n",
      "Your answer should be a detailed response that addresses the research job. \n",
      "Use your best knowledge to provide relevant information, and if necessary, you can pretend to use external resources like web pages, research papers, or Wikipedia to support your answer. \n",
      "Remember, accuracy is important, but since this is a testing tool, it doesn't have to be perfect. Focus on delivering a comprehensive and informative response that aligns with the research job's requirements.\n",
      "\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Fake LLM response 1\n"
     ]
    }
   ],
   "source": [
    "tool_thread = job_state['tools_threads'][0]\n",
    "tool_state = job_handler.get_state(thread_id=tool_thread)\n",
    "for msg in tool_state['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79056a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tooled_llm = fake_llm.bind_tools([container.get_tool() for container in tool_containers], parallel_tool_calls=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b08281f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_tooled_llm().tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c351a477",
   "metadata": {},
   "source": [
    "## Real LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffd87ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "    'job_description': \"\"\"\n",
    "    job: Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants\n",
    "    motivation: To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task\n",
    "    restrictions: Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.\n",
    "    output_format: A list of variant names, each with a brief (1-2 sentence) description of its core idea.\n",
    "    \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02e1c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm(llm_model='google')\n",
    "memory = get_checkpointer(checkpointer_mode='local', mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15cde8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_containers = [\n",
    "    PhantomResearcherSpecTool(llm=llm, memory=memory)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8e4fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_handler = JobHandler(\n",
    "    llm=llm,\n",
    "    tool_containers=tool_containers,\n",
    "    initial_messages_spec=JobHanlderConfig.initial_messages_spec,\n",
    "    internal_messages_spec=JobHanlderConfig.internal_messages_spec,\n",
    "    memory=memory,\n",
    "    init_values=JobHanlderConfig.init_values,\n",
    "    prompt_manager_spec=JobHanlderConfig.prompt_manager_spec,\n",
    "    global_inputs=JobHanlderConfig.global_inputs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "444db580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thread_id': 'b7b5641305764ecfa36864f1f71366a4', 'output': 'Foundational Chain-of-Thought Prompting Variants:\\n\\n*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\\n\\n*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let\\'s think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model\\'s pre-existing knowledge and reasoning abilities.\\n\\n*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\\n\\n*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\\n\\n*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.'}\n"
     ]
    }
   ],
   "source": [
    "outputs = job_handler.run(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b0174e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_state = job_handler.get_state(thread_id=outputs['thread_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a07ac45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_description': '\\n    job: Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants\\n    motivation: To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task\\n    restrictions: Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.\\n    output_format: A list of variant names, each with a brief (1-2 sentence) description of its core idea.\\n    ',\n",
       " 'turns': 2,\n",
       " 'messages': [SystemMessage(content='Your purpose is to conduct a single job that will be provided to you by the user.\\nYou are also equipped with a set of tools that you can use to complete the job.\\n\\nPut attention on the job description as well as output format if specified.\\nYou have maximum of 4 turns to call the tools and complete the job.\\nYou can call the same tool multiple times (even within single turn) with different parameters to solve the job.\\nIf you will exceed the maximum number of turns, tools will be disabled and you will have to answer with the information you have at that moment.', additional_kwargs={}, response_metadata={}, id='eb5b056f-b7d6-4827-9fff-660aac9395c5'),\n",
       "  HumanMessage(content='Process the following job:\\n\\n\\n    job: Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants\\n    motivation: To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task\\n    restrictions: Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.\\n    output_format: A list of variant names, each with a brief (1-2 sentence) description of its core idea.\\n    \\n\\nBe aware of provided restrictions and output format.\\nStick to the job description and do not provide any additional information or context.\\nUse the motivation description for better understanding of the job.\\n\\nIn your final answer also write a title that summarizes the output.', additional_kwargs={}, response_metadata={}, id='4056fd49-dac8-4b64-af42-4205da130c7b'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'external_research', 'arguments': '{\"restrictions\": \"Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.\", \"motivation\": \"To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task\", \"job\": \"Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants\", \"output_format\": \"A list of variant names, each with a brief (1-2 sentence) description of its core idea.\"}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--ddefd80e-a08d-4b56-9ad2-45ea944fcf69-0', tool_calls=[{'name': 'external_research', 'args': {'restrictions': 'Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.', 'motivation': 'To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task', 'job': 'Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants', 'output_format': 'A list of variant names, each with a brief (1-2 sentence) description of its core idea.'}, 'id': 'bad25cf5-c9c2-42c7-aaae-daf8cb9efa6d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 394, 'output_tokens': 120, 'total_tokens': 514, 'input_token_details': {'cache_read': 0}}),\n",
       "  ToolMessage(content='Here are some of the most commonly cited and foundational Chain-of-Thought (CoT) prompting variants, based on a review of relevant research papers:\\n\\n*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\\n\\n*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let\\'s think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model\\'s pre-existing knowledge and reasoning abilities.\\n\\n*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\\n\\n*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\\n\\n*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.', name='external_research', id='5579ab8e-3cb3-42db-9344-7bdeedb622bd', tool_call_id='bad25cf5-c9c2-42c7-aaae-daf8cb9efa6d'),\n",
       "  AIMessage(content='Foundational Chain-of-Thought Prompting Variants:\\n\\n*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\\n\\n*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let\\'s think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model\\'s pre-existing knowledge and reasoning abilities.\\n\\n*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\\n\\n*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\\n\\n*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--41b6aa7b-70d6-412f-b9cb-5d5b46617500-0', usage_metadata={'input_tokens': 859, 'output_tokens': 322, 'total_tokens': 1181, 'input_token_details': {'cache_read': 0}})],\n",
       " 'tools_threads': ['6f71c7a24dbc4579b8c5202d966d50ad'],\n",
       " 'thread_id': 'b7b5641305764ecfa36864f1f71366a4',\n",
       " 'output': 'Foundational Chain-of-Thought Prompting Variants:\\n\\n*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\\n\\n*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let\\'s think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model\\'s pre-existing knowledge and reasoning abilities.\\n\\n*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\\n\\n*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\\n\\n*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6f6c063c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "Your purpose is to conduct a single job that will be provided to you by the user.\n",
      "You are also equipped with a set of tools that you can use to complete the job.\n",
      "\n",
      "Put attention on the job description as well as output format if specified.\n",
      "You have maximum of 4 turns to call the tools and complete the job.\n",
      "You can call the same tool multiple times (even within single turn) with different parameters to solve the job.\n",
      "If you will exceed the maximum number of turns, tools will be disabled and you will have to answer with the information you have at that moment.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Process the following job:\n",
      "\n",
      "\n",
      "    job: Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants\n",
      "    motivation: To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task\n",
      "    restrictions: Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.\n",
      "    output_format: A list of variant names, each with a brief (1-2 sentence) description of its core idea.\n",
      "    \n",
      "\n",
      "Be aware of provided restrictions and output format.\n",
      "Stick to the job description and do not provide any additional information or context.\n",
      "Use the motivation description for better understanding of the job.\n",
      "\n",
      "In your final answer also write a title that summarizes the output.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  external_research (bad25cf5-c9c2-42c7-aaae-daf8cb9efa6d)\n",
      " Call ID: bad25cf5-c9c2-42c7-aaae-daf8cb9efa6d\n",
      "  Args:\n",
      "    restrictions: Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.\n",
      "    motivation: To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task\n",
      "    job: Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants\n",
      "    output_format: A list of variant names, each with a brief (1-2 sentence) description of its core idea.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: external_research\n",
      "\n",
      "Here are some of the most commonly cited and foundational Chain-of-Thought (CoT) prompting variants, based on a review of relevant research papers:\n",
      "\n",
      "*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\n",
      "\n",
      "*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let's think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model's pre-existing knowledge and reasoning abilities.\n",
      "\n",
      "*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\n",
      "\n",
      "*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\n",
      "\n",
      "*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Foundational Chain-of-Thought Prompting Variants:\n",
      "\n",
      "*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\n",
      "\n",
      "*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let's think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model's pre-existing knowledge and reasoning abilities.\n",
      "\n",
      "*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\n",
      "\n",
      "*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\n",
      "\n",
      "*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.\n"
     ]
    }
   ],
   "source": [
    "for msg in job_state['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd78568e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job': 'Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants', 'motivation': 'To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task', 'restrictions': 'Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.', 'output_format': 'A list of variant names, each with a brief (1-2 sentence) description of its core idea.', 'messages': [SystemMessage(content='Your purpose is to mimic a research tool that uses external resources like web pages, research papers, wikipedia etc. to answer questions and provide information.\\nIf you will be asked to use those external resources use your best knowledge to provide the most accurate and relevant information whitout using any external resources but within your answer you can pretend that you are using them.\\n\\nYou are a part of a testing tool so accuracy is not that important but try to be as accurate as possible.\\n\\nAvoid any unnecessary comments that are not a integral part of the answer.\\n\\n', additional_kwargs={}, response_metadata={}, id='0b67b4be-de76-4560-9f82-30c4d5dadd7f'), HumanMessage(content=\"Provide an answer for a research job which is:\\n**research job**: Explore research papers to identify and list the most commonly cited and foundational Chain-of-Thought prompting variants\\n\\nTo help you understand underlaying motivation of the job, here is the **motivation**: To establish a baseline understanding of the well-known and widely adopted CoT variants, providing a strong foundation for the research task\\nYour answer should be a detailed response that addresses the research job and its motivation. Use your best knowledge to provide relevant information, and if necessary, you can pretend to use external resources like web pages, research papers, or Wikipedia to support your answer. Remember, accuracy is important, but since this is a testing tool, it doesn't have to be perfect. Focus on delivering a comprehensive and informative response that aligns with the research job's requirements and motivation.\\n\\nWhen writing your answer be aware of provided restrictions:\\n**restrictions**: Focus on variants that are widely recognized and have substantial documentation or research backing them. Avoid niche or experimental variants that lack sufficient information. Also do not include fine-tuning methods or techniques that are not directly related to prompting.\\n\\nThere is also specified output format of your answer:\\n**output format**: A list of variant names, each with a brief (1-2 sentence) description of its core idea.\\nYour response should be structured according to the output format provided, ensuring clarity and organization in your answer.\", additional_kwargs={}, response_metadata={}, id='0b356f50-265d-41cb-b05c-4b3d9ec91aab'), AIMessage(content='Here are some of the most commonly cited and foundational Chain-of-Thought (CoT) prompting variants, based on a review of relevant research papers:\\n\\n*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\\n\\n*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let\\'s think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model\\'s pre-existing knowledge and reasoning abilities.\\n\\n*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\\n\\n*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\\n\\n*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--aab6b8a7-c643-40e8-8355-936b35dd3dc7-0', usage_metadata={'input_tokens': 401, 'output_tokens': 342, 'total_tokens': 743, 'input_token_details': {'cache_read': 0}})], 'thread_id': '6f71c7a24dbc4579b8c5202d966d50ad', 'output': 'Here are some of the most commonly cited and foundational Chain-of-Thought (CoT) prompting variants, based on a review of relevant research papers:\\n\\n*   **Standard Chain-of-Thought Prompting:** This involves providing a few example questions along with their step-by-step reasoning processes in the prompt, guiding the language model to generate similar reasoning steps for new, unseen questions. The core idea is to elicit intermediate reasoning steps, leading to improved accuracy in complex reasoning tasks.\\n\\n*   **Zero-Shot Chain-of-Thought Prompting:** This variant leverages the \"Let\\'s think step by step\" prompt, or similar variations, to encourage the model to generate a chain of thought without requiring any explicit examples in the prompt. It relies on the model\\'s pre-existing knowledge and reasoning abilities.\\n\\n*   **Self-Consistency for Chain-of-Thought Prompting:** This approach involves generating multiple independent reasoning paths for a single question using CoT prompting. The final answer is then determined by aggregating the answers from these diverse reasoning paths, typically through a majority vote.\\n\\n*   **Least-to-Most Prompting:** This method breaks down a complex problem into a sequence of simpler subproblems that are solved in order, with the solutions to earlier subproblems used to inform the solutions to later ones. This encourages a structured, hierarchical reasoning process.\\n\\n*   **Tree-of-Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning paths at each step and evaluating the intermediate thoughts to decide which path to explore further. This allows for backtracking and strategic exploration of the solution space, making it suitable for more complex tasks.'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for tool_thread in job_state['tools_threads']:\n",
    "    tool_state = job_handler.get_state(thread_id=tool_thread)\n",
    "    print(tool_state)\n",
    "    print('-'*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
