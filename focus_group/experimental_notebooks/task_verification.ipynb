{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ae396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"focus_group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e927141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.chatbots.chatbot import BaseChatBot\n",
    "from typing import Dict, Any, List, Annotated\n",
    "from pydantic import BaseModel\n",
    "from langgraph.constants import Send\n",
    "from operator import add\n",
    "from fcgb.types.research import SimpleTaskModel\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "#from fcgb.chatbots.selfconv import SimpleTaskDistributionChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c3eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTaskDistributionChatBot(BaseChatBot):\n",
    "    \"\"\"\n",
    "    A chatbot that distributes tasks for a single LLM answer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            llm,\n",
    "            initial_messages_spec,\n",
    "            internal_messages_spec,\n",
    "            memory,\n",
    "            global_inputs={},\n",
    "            init_values={},\n",
    "            prompt_manager_spec = {}\n",
    "        ):\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            initial_messages_spec=initial_messages_spec,\n",
    "            internal_messages_spec=internal_messages_spec,\n",
    "            memory=memory,\n",
    "            init_values=init_values,\n",
    "            prompt_manager_spec=prompt_manager_spec,\n",
    "            global_inputs=global_inputs,\n",
    "            compile=False\n",
    "            )\n",
    "\n",
    "        self.compile_graph()\n",
    "\n",
    "    def _set_state_class(self):\n",
    "\n",
    "        task_list_model = self.internal_messages_spec['task_list']['answer_format']\n",
    "        task_answer_model = self.internal_messages_spec['task']['answer_format']\n",
    "        answer_model = self.internal_messages_spec['answer']['answer_format']\n",
    "\n",
    "        class VerificationState(BaseModel):\n",
    "            parent_thread_id: str \n",
    "            template_inputs: Dict[str, str]\n",
    "            task_list: task_list_model | None # type: ignore\n",
    "            simple_task_response: Annotated[List[task_answer_model], add] # type: ignore\n",
    "            verified_answer: answer_model | None # type: ignore\n",
    "\n",
    "        self.state_class = VerificationState\n",
    "\n",
    "    def _set_tasks_gen_func(self):\n",
    "        \"\"\"\n",
    "        Set the function that generates the verification tasks.\n",
    "        \"\"\"\n",
    "        \n",
    "        def tasks_gen_func(state: self.state_class) -> Dict: # type: ignore\n",
    "            \n",
    "            task_list = self._invoke_internal_msg(name='task_list', template_inputs=state.template_inputs)\n",
    "\n",
    "            return {'task_list': task_list}\n",
    "        \n",
    "        return tasks_gen_func\n",
    "        \n",
    "    def _set_tasks_router_func(self):\n",
    "        \"\"\"\n",
    "        Set the function that distributes tasks along nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        def tasks_router_func(state: self.state_class) -> SimpleTaskModel: # type: ignore\n",
    "\n",
    "            return [Send('task_node',\n",
    "                        {\n",
    "                        'template_inputs': state.template_inputs,\n",
    "                        'task': task,\n",
    "                        'simple_task_response': None\n",
    "                        }) for task in state.task_list.prompts]\n",
    "        \n",
    "        return tasks_router_func\n",
    "    \n",
    "    def _set_task_node_func(self):\n",
    "        \"\"\"\n",
    "        Set the function that solve the tasks.\n",
    "        \"\"\"\n",
    "        \n",
    "        def task_node_func(state: SimpleTaskModel) -> Dict:\n",
    "\n",
    "            system = self.internal_prompts['task']['prompt'].format(**state['template_inputs'], **self.global_inputs)\n",
    "            prompt = state['task'].format(**state['template_inputs'], **self.global_inputs)\n",
    "\n",
    "            messages = [SystemMessage(content=system), HumanMessage(content=prompt)]\n",
    "            task_response = self.llm.with_structured_output(self.internal_prompts['task']['answer_format']).invoke(messages)\n",
    "\n",
    "            return {'simple_task_response': [task_response]}\n",
    "        \n",
    "        return task_node_func\n",
    "    \n",
    "    def _set_output_node_func(self):\n",
    "\n",
    "        def output_node_func(state: self.state_class) -> Dict: # type: ignore\n",
    "\n",
    "            task_response = '\\n-----\\n'.join([f'Job {i+1}:\\n'+resp.recommendations for i, resp in enumerate(state.simple_task_response)])\n",
    "\n",
    "            output = self._invoke_internal_msg(name='answer', template_inputs=state.template_inputs | {'task_response': task_response})\n",
    "\n",
    "            return {'verified_answer': output}\n",
    "        \n",
    "        return output_node_func\n",
    "    \n",
    "    def _compile_graph(self):\n",
    "\n",
    "        task_gen_func = self._set_tasks_gen_func()\n",
    "        tasks_router_func = self._set_tasks_router_func()\n",
    "        task_node_func = self._set_task_node_func()\n",
    "        output_node_func = self._set_output_node_func()\n",
    "\n",
    "        workflow = StateGraph(self.state_class)\n",
    "        workflow.add_node('task_gen', task_gen_func)\n",
    "        workflow.add_node('task_node', task_node_func)\n",
    "        workflow.add_node('output_node', output_node_func)\n",
    "\n",
    "        workflow.add_edge(START, 'task_gen')\n",
    "        workflow.add_conditional_edges('task_gen', tasks_router_func, ['task_node'])\n",
    "        workflow.add_edge('task_node', 'output_node')\n",
    "        workflow.add_edge('output_node', END)\n",
    "\n",
    "        self.graph = workflow.compile(\n",
    "            checkpointer=self.memory,\n",
    "        )\n",
    "\n",
    "    def run(self, template_inputs: Dict, thread_id: str, parent_thread_id: str):\n",
    "\n",
    "        self.init_thread(thread_id=thread_id, template_inputs=template_inputs)\n",
    "\n",
    "        config = self._get_config(thread_id)\n",
    "\n",
    "        return self.graph.invoke({'parent_thread_id': parent_thread_id, 'template_inputs': template_inputs}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3d84b",
   "metadata": {},
   "source": [
    "## fake llm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb2c0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_research_inputs = {\n",
    "    'template_inputs': {\n",
    "        'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
    "        'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.',\n",
    "    },\n",
    "    'ssc_thread_id': 'test_2/ssc1',\n",
    "    'parent_thread_id': 'test_2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72021fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.chatbots.specbots import SelfConvForStrategySpecBot, StrategizedSelfResearchSpecBot\n",
    "from fcgb.fake_models import FakeLLM\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "fake_llm = FakeLLM()\n",
    "\n",
    "memory = MemorySaver()\n",
    "convbot = SelfConvForStrategySpecBot(llm=fake_llm, memory=memory)\n",
    "strategybot = StrategizedSelfResearchSpecBot(llm=fake_llm, self_conv_bot=convbot, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48e9dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
       "  'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.'},\n",
       " 'ssc_thread_id': 'test_2/ssc1',\n",
       " 'parent_thread_id': 'test_2',\n",
       " 'ssc_summary': SimpleAnswerModel(answer='Fake string srxrs'),\n",
       " 'strategies': StrategyTaskModel(strategies=[{'strategy_description': 'Fake string vywvv', 'paraphrased_task': 'Fake string drije', 'paraphrased_context': 'Fake string pnuhd'}, {'strategy_description': 'Fake string bdwql', 'paraphrased_task': 'Fake string aaouo', 'paraphrased_context': 'Fake string fijkg'}, {'strategy_description': 'Fake string upjna', 'paraphrased_task': 'Fake string ibxnn', 'paraphrased_context': 'Fake string jwxsu'}, {'strategy_description': 'Fake string belfk', 'paraphrased_task': 'Fake string faehz', 'paraphrased_context': 'Fake string dqrea'}]),\n",
       " 'sc_thread_id': ['test_2/ssc1/self_conv0',\n",
       "  'test_2/ssc1/self_conv1',\n",
       "  'test_2/ssc1/self_conv2',\n",
       "  'test_2/ssc1/self_conv3'],\n",
       " 'sc_summary': ['Fake string komfq',\n",
       "  'Fake string wfynb',\n",
       "  'Fake string rtiza',\n",
       "  'Fake string omqhb']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_output = strategybot.run(**self_research_inputs)\n",
    "research_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07af7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_inputs ={\n",
    "    'template_inputs': self_research_inputs['template_inputs'] | {'answer': research_output['ssc_summary'].answer},\n",
    "    'thread_id': self_research_inputs['ssc_thread_id'] + '/ver',\n",
    "    'parent_thread_id': self_research_inputs['ssc_thread_id']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c62b6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.cfg.chat_inputs_spec import ResearchVerificationConfig\n",
    "\n",
    "verificationbot = SimpleTaskDistributionChatBot(\n",
    "    llm=fake_llm,\n",
    "    initial_messages_spec=ResearchVerificationConfig.initial_messages_spec,\n",
    "    internal_messages_spec=ResearchVerificationConfig.internal_messages_spec,\n",
    "    memory=memory,\n",
    "    global_inputs=ResearchVerificationConfig.global_inputs,\n",
    "    init_values=ResearchVerificationConfig.init_values,\n",
    "    prompt_manager_spec=ResearchVerificationConfig.prompt_manager_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7377ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parent_thread_id': 'test_2/ssc1',\n",
       " 'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
       "  'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.',\n",
       "  'answer': 'Fake string srxrs'},\n",
       " 'task_list': PromptTemplatesListModel(analysis='Fake string ukgjw', prompts=['Fake string klyze', 'Fake string jlaxq', 'Fake string janjk']),\n",
       " 'simple_task_response': [SingleVerificationModel(analysis='Fake string dvqxz', recommendations='Fake string cvqeq'),\n",
       "  SingleVerificationModel(analysis='Fake string apynn', recommendations='Fake string nuqma'),\n",
       "  SingleVerificationModel(analysis='Fake string xeudp', recommendations='Fake string suvmg')],\n",
       " 'verified_answer': SimpleAnswerModel(answer='Fake string vzojy')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verification_output = verificationbot.run(**verification_inputs)\n",
    "verification_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bee1c5",
   "metadata": {},
   "source": [
    "## real llm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b6c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.cfg.precompiled import get_checkpointer, get_llm\n",
    "\n",
    "memory = get_checkpointer(checkpointer_mode='mongodb', mode='test')\n",
    "llm = get_llm(llm_model='google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ade27e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.chatbots.specbots import SelfConvForStrategySpecBot, StrategizedSelfResearchSpecBot\n",
    "from fcgb.cfg.chat_inputs_spec import ResearchVerificationConfig\n",
    "\n",
    "convbot = SelfConvForStrategySpecBot(llm=llm, memory=memory)\n",
    "strategybot = StrategizedSelfResearchSpecBot(llm=llm, self_conv_bot=convbot, memory=memory)\n",
    "verificationbot = SimpleTaskDistributionChatBot(\n",
    "    llm=llm,\n",
    "    initial_messages_spec=ResearchVerificationConfig.initial_messages_spec,\n",
    "    internal_messages_spec=ResearchVerificationConfig.internal_messages_spec,\n",
    "    memory=memory,\n",
    "    global_inputs=ResearchVerificationConfig.global_inputs,\n",
    "    init_values=ResearchVerificationConfig.init_values,\n",
    "    prompt_manager_spec=ResearchVerificationConfig.prompt_manager_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from fcgb.cfg.precompiled import get_db_client\n",
    "\n",
    "db = get_db_client(db_engine='mongodb', mode='test')\n",
    "\n",
    "db['checkpoints_writes'].delete_many({'thread_id': 'partial/research/cot/ssc1'})\n",
    "db['checkpoints'].delete_many({'thread_id': 'partial/research/cot/ssc1'})\n",
    "\n",
    "strategybot.get_state('partial/research/cot/ssc1')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fac4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_research_inputs = {\n",
    "    'template_inputs': {\n",
    "        'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
    "        'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. List should be very comprehensive, be sure to include all known variants of CoT prompting techniques. Be aware to not include fine-tuning methods, as they are not related to prompting.',\n",
    "    },\n",
    "    'ssc_thread_id': 'partial/research/cot/ssc7',\n",
    "    'parent_thread_id': 'partial/research/cot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19bac65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
       "  'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. List should be very comprehensive, be sure to include all known variants of CoT prompting techniques.'},\n",
       " 'ssc_thread_id': 'partial/research/cot/ssc7',\n",
       " 'parent_thread_id': 'partial/research/cot',\n",
       " 'ssc_summary': SimpleAnswerModel(answer='The variants of Chain-of-Thought (CoT) prompting techniques are:\\n\\n*   Standard CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Finetuned CoT\\n*   Least-to-Most prompting\\n*   Automatic Prompt Optimization (APO)\\n*   CoT with Verification\\n*   Graph CoT\\n*   Tree-of-Thoughts (ToT)\\n*   CoT combined with Knowledge Retrieval\\n*   Distillation for CoT\\n*   Complexity-Based Prompting\\n*   Task-Specific CoT\\n*   Self-Refinement\\n*   CoT with External Verification\\n*   Constraint-Based CoT\\n*   Graph-of-Thoughts (GoT)\\n*   Program-of-Thoughts (PoT)\\n*   Chain-of-Hindsight (CoH)\\n*   CoT-Guided ToT/GoT\\n*   CoT for Branch Selection in ToT\\n*   CoT for Node Generation in GoT'),\n",
       " 'strategies': StrategyTaskModel(strategies=[{'strategy_description': 'Comprehensive Recall: Focus on recalling and listing all known CoT variants. Prioritize breadth of coverage and aim for an exhaustive list.', 'paraphrased_task': 'Could you provide a detailed list of all Chain-of-Thought (CoT) prompting techniques?', 'paraphrased_context': \"I'm writing a chapter about Chain-of-Thought (CoT) prompting and need a complete list of all its variations.\"}, {'strategy_description': 'Taxonomy and Classification: Focus on classifying CoT techniques into different categories or types. Aim to provide a structured overview.', 'paraphrased_task': 'What are the different methods and approaches within Chain-of-Thought (CoT) prompting?', 'paraphrased_context': 'I am trying to understand the different ways Chain-of-Thought prompting can be applied.'}, {'strategy_description': 'Exploration of Novel Techniques: Focus on uncovering less common or experimental CoT variants. Prioritize depth of exploration and aim to identify cutting-edge techniques.', 'paraphrased_task': 'Can you list all Chain-of-Thought (CoT) prompting methods, including less common or experimental ones?', 'paraphrased_context': \"I'm researching Chain-of-Thought prompting for a specific application and need to know all the available options.\"}]),\n",
       " 'sc_thread_id': ['partial/research/cot/ssc7/self_conv0',\n",
       "  'partial/research/cot/ssc7/self_conv1',\n",
       "  'partial/research/cot/ssc7/self_conv2'],\n",
       " 'sc_summary': ['*   Standard CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Finetuned CoT\\n*   Least-to-Most prompting\\n*   Automatic Prompt Optimization (APO)\\n*   CoT with Verification\\n*   Graph CoT\\n*   Tree-of-Thoughts (ToT)\\n*   CoT combined with Knowledge Retrieval\\n*   Distillation for CoT\\n*   Complexity-Based Prompting\\n*   Task-Specific CoT\\n*   Self-Refinement\\n*   CoT with External Verification\\n*   Constraint-Based CoT',\n",
       "  'The variants of Chain-of-Thought (CoT) prompting techniques are:\\n\\n- Standard CoT\\n- Self-Consistency\\n- Zero-shot CoT\\n- Least-to-Most prompting\\n- Tree-of-Thoughts (ToT) ',\n",
       "  \"Here's a comprehensive list of Chain-of-Thought (CoT) prompting techniques:\\n\\n*   Standard Chain-of-Thought (CoT)\\n*   Self-Consistency Decoding\\n*   Graph-of-Thoughts (GoT)\\n*   Tree-of-Thoughts (ToT)\\n*   Program-of-Thoughts (PoT)\\n*   Chain-of-Hindsight (CoH)\\n*   CoT-Guided ToT/GoT\\n*   CoT for Branch Selection in ToT\\n*   CoT for Node Generation in GoT\"]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_output = strategybot.run(**self_research_inputs)\n",
    "research_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835c799b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
       "  'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. List should be very comprehensive, be sure to include all known variants of CoT prompting techniques.'},\n",
       " 'ssc_thread_id': 'partial/research/cot/ssc7',\n",
       " 'parent_thread_id': 'partial/research/cot',\n",
       " 'ssc_summary': SimpleAnswerModel(answer='The variants of Chain-of-Thought (CoT) prompting techniques are:\\n\\n*   Standard CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Finetuned CoT\\n*   Least-to-Most prompting\\n*   Automatic Prompt Optimization (APO)\\n*   CoT with Verification\\n*   Graph CoT\\n*   Tree-of-Thoughts (ToT)\\n*   CoT combined with Knowledge Retrieval\\n*   Distillation for CoT\\n*   Complexity-Based Prompting\\n*   Task-Specific CoT\\n*   Self-Refinement\\n*   CoT with External Verification\\n*   Constraint-Based CoT\\n*   Graph-of-Thoughts (GoT)\\n*   Program-of-Thoughts (PoT)\\n*   Chain-of-Hindsight (CoH)\\n*   CoT-Guided ToT/GoT\\n*   CoT for Branch Selection in ToT\\n*   CoT for Node Generation in GoT'),\n",
       " 'strategies': StrategyTaskModel(strategies=[{'strategy_description': 'Comprehensive Recall: Focus on recalling and listing all known CoT variants. Prioritize breadth of coverage and aim for an exhaustive list.', 'paraphrased_task': 'Could you provide a detailed list of all Chain-of-Thought (CoT) prompting techniques?', 'paraphrased_context': \"I'm writing a chapter about Chain-of-Thought (CoT) prompting and need a complete list of all its variations.\"}, {'strategy_description': 'Taxonomy and Classification: Focus on classifying CoT techniques into different categories or types. Aim to provide a structured overview.', 'paraphrased_task': 'What are the different methods and approaches within Chain-of-Thought (CoT) prompting?', 'paraphrased_context': 'I am trying to understand the different ways Chain-of-Thought prompting can be applied.'}, {'strategy_description': 'Exploration of Novel Techniques: Focus on uncovering less common or experimental CoT variants. Prioritize depth of exploration and aim to identify cutting-edge techniques.', 'paraphrased_task': 'Can you list all Chain-of-Thought (CoT) prompting methods, including less common or experimental ones?', 'paraphrased_context': \"I'm researching Chain-of-Thought prompting for a specific application and need to know all the available options.\"}]),\n",
       " 'sc_thread_id': ['partial/research/cot/ssc7/self_conv0',\n",
       "  'partial/research/cot/ssc7/self_conv1',\n",
       "  'partial/research/cot/ssc7/self_conv2'],\n",
       " 'sc_summary': ['*   Standard CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Finetuned CoT\\n*   Least-to-Most prompting\\n*   Automatic Prompt Optimization (APO)\\n*   CoT with Verification\\n*   Graph CoT\\n*   Tree-of-Thoughts (ToT)\\n*   CoT combined with Knowledge Retrieval\\n*   Distillation for CoT\\n*   Complexity-Based Prompting\\n*   Task-Specific CoT\\n*   Self-Refinement\\n*   CoT with External Verification\\n*   Constraint-Based CoT',\n",
       "  'The variants of Chain-of-Thought (CoT) prompting techniques are:\\n\\n- Standard CoT\\n- Self-Consistency\\n- Zero-shot CoT\\n- Least-to-Most prompting\\n- Tree-of-Thoughts (ToT) ',\n",
       "  \"Here's a comprehensive list of Chain-of-Thought (CoT) prompting techniques:\\n\\n*   Standard Chain-of-Thought (CoT)\\n*   Self-Consistency Decoding\\n*   Graph-of-Thoughts (GoT)\\n*   Tree-of-Thoughts (ToT)\\n*   Program-of-Thoughts (PoT)\\n*   Chain-of-Hindsight (CoH)\\n*   CoT-Guided ToT/GoT\\n*   CoT for Branch Selection in ToT\\n*   CoT for Node Generation in GoT\"]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_output = strategybot.get_state(self_research_inputs['ssc_thread_id'])\n",
    "research_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a420c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variants of Chain-of-Thought (CoT) prompting techniques are:\n",
      "\n",
      "*   Standard CoT\n",
      "*   Zero-shot CoT\n",
      "*   Self-Consistency\n",
      "*   Finetuned CoT\n",
      "*   Least-to-Most prompting\n",
      "*   Automatic Prompt Optimization (APO)\n",
      "*   CoT with Verification\n",
      "*   Graph CoT\n",
      "*   Tree-of-Thoughts (ToT)\n",
      "*   CoT combined with Knowledge Retrieval\n",
      "*   Distillation for CoT\n",
      "*   Complexity-Based Prompting\n",
      "*   Task-Specific CoT\n",
      "*   Self-Refinement\n",
      "*   CoT with External Verification\n",
      "*   Constraint-Based CoT\n",
      "*   Graph-of-Thoughts (GoT)\n",
      "*   Program-of-Thoughts (PoT)\n",
      "*   Chain-of-Hindsight (CoH)\n",
      "*   CoT-Guided ToT/GoT\n",
      "*   CoT for Branch Selection in ToT\n",
      "*   CoT for Node Generation in GoT\n"
     ]
    }
   ],
   "source": [
    "for line in research_output['ssc_summary'].answer.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ea647bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_inputs ={\n",
    "    'template_inputs': self_research_inputs['template_inputs'] | {'answer': research_output['ssc_summary'].answer},\n",
    "    'thread_id': self_research_inputs['ssc_thread_id'] + '/ver12',\n",
    "    'parent_thread_id': self_research_inputs['ssc_thread_id']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3ca85b0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'recommendations'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m verification_output = \u001b[43mverificationbot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mverification_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m verification_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/guide_builder/focus_group/fcgb/src/fcgb/chatbots/selfconv.py:400\u001b[39m, in \u001b[36mSimpleTaskDistributionChatBot.run\u001b[39m\u001b[34m(self, template_inputs, thread_id, parent_thread_id)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28mself\u001b[39m.init_thread(thread_id=thread_id, template_inputs=template_inputs)\n\u001b[32m    398\u001b[39m config = \u001b[38;5;28mself\u001b[39m._get_config(thread_id)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mparent_thread_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent_thread_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtemplate_inputs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/guide_builder/lang_venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2823\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2820\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2821\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2823\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2824\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2825\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2827\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2828\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2829\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2832\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2833\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2834\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2835\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2836\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2837\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2838\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/guide_builder/lang_venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2461\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2455\u001b[39m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2456\u001b[39m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[32m   2457\u001b[39m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2458\u001b[39m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2459\u001b[39m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[32m   2460\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2462\u001b[39m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2463\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2464\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2465\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2466\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2468\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/guide_builder/focus_group/fcgb/src/fcgb/chatbots/selfconv.py:365\u001b[39m, in \u001b[36mSimpleTaskDistributionChatBot._set_output_node_func.<locals>.output_node_func\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moutput_node_func\u001b[39m(state: \u001b[38;5;28mself\u001b[39m.state_class) -> Dict: \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     task_response = \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-----\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join([\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mJob \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m+\u001b[43mresp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecommendations\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(state.simple_task_response)])\n\u001b[32m    367\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._invoke_internal_msg(name=\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m, template_inputs=state.template_inputs | {\u001b[33m'\u001b[39m\u001b[33mtask_response\u001b[39m\u001b[33m'\u001b[39m: task_response})\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mverified_answer\u001b[39m\u001b[33m'\u001b[39m: output}\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'recommendations'",
      "During task with name 'output_node' and id 'cc6224b3-43dd-a559-e20d-55f6a12d05be'"
     ]
    }
   ],
   "source": [
    "verification_output = verificationbot.run(**verification_inputs)\n",
    "verification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cbfa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Task Decomposition:** The task is to list all variants of Chain-of-Thought (CoT) prompting techniques. The answer format is a list.\n",
      "\n",
      "2. **Contextual Restrictions and Information:**\n",
      "    - The list should be comprehensive.\n",
      "    - The list should not include fine-tuning methods.\n",
      "    - The list should only include prompting techniques.\n",
      "\n",
      "3. **Answer Analysis and Doubts:**\n",
      "    - The answer provides a list of CoT prompting techniques. However, some entries might be fine-tuning methods (which should be excluded based on the context). Also, the comprehensiveness of the list is questionable and needs verification. There might be missing CoT variants. Some listed items may be duplicates or very similar techniques described with different names. The correctness of each listed item needs to be verified to ensure it's a valid CoT prompting technique and not a misclassified or non-existent method.\n",
      "    - Some of the items might be too general, e.g. 'CoT combined with Knowledge Retrieval' is a very broad category, and it would be better to list concrete techniques. Also, some items seem to be follow-up research directions instead of established CoT variants.\n"
     ]
    }
   ],
   "source": [
    "# task delegation analysis\n",
    "for line in verification_output['task_list'].analysis.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868972f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job: Verify that each item in the list is a valid Chain-of-Thought (CoT) *prompting* technique and not a fine-tuning method or something else.\n",
      "Doubts: Some items might be misclassified or not directly related to CoT prompting as defined in the context.\n",
      "Additional Info: The context specifies that fine-tuning methods should be excluded. Focus on techniques that guide the LLM during inference, not during training.\n",
      "Analysis methodology: 1. For each item in the list, research its definition and usage.\n",
      "2. Determine if the item is a prompting technique or a fine-tuning method.\n",
      "3. If it's a prompting technique, verify that it aligns with the Chain-of-Thought approach (i.e., involves generating intermediate reasoning steps).\n",
      "4. If it's not a valid CoT prompting technique, flag it for removal or modification.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Job: Check the comprehensiveness of the list. Are there any significant and well-known CoT prompting techniques missing?\n",
      "Doubts: The list might not be exhaustive, and some important variants could be absent.\n",
      "Additional Info: Consider recent research papers and surveys on CoT prompting to identify potential omissions.\n",
      "Analysis methodology: 1. Search for recent surveys and research papers on Chain-of-Thought prompting.\n",
      "2. Identify CoT variants mentioned in these sources that are not present in the provided list.\n",
      "3. Add any missing variants to the list.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Job: Identify and remove any duplicate or very similar CoT prompting techniques that are listed under different names.\n",
      "Doubts: The list might contain redundant entries that describe essentially the same technique.\n",
      "Additional Info: Pay close attention to the descriptions and underlying mechanisms of each listed item.\n",
      "Analysis methodology: 1. Compare the descriptions of each pair of items in the list.\n",
      "2. If two items describe the same technique with different names, choose the most common or descriptive name and remove the other item.\n",
      "3. Consider merging similar items into a single entry if appropriate.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Job: Verify the accuracy of the names of the techniques. Some of them seems to be not correct, e.g. 'Finetuned CoT'.\n",
      "Doubts: Some of the names are not standard and can be misleading.\n",
      "Additional Info: Check original papers for the correct names.\n",
      "Analysis methodology: 1. Search for original papers describing the mentioned techniques.\n",
      "2. Compare names from the list with names used in the original papers.\n",
      "3. Correct names if needed.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Job: Some of the items might be too general, e.g. 'CoT combined with Knowledge Retrieval'. Try to be more specific.\n",
      "Doubts: General items should be replaced with specific techniques.\n",
      "Additional Info: Focus on concrete techniques instead of general categories.\n",
      "Analysis methodology: 1. Identify general items.\n",
      "2. Find specific techniques that fall under these general categories.\n",
      "3. Replace general items with specific techniques.\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompts list\n",
    "sep = f'\\n{'-' * 40}\\n'\n",
    "for sc_summ in verification_output['task_list'].prompts:\n",
    "    for line in sc_summ.split('\\n'):\n",
    "        print(line)\n",
    "    print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09213f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will analyze each item in the provided list to determine if it is a valid Chain-of-Thought (CoT) prompting technique, ensuring it is not a fine-tuning method or an unrelated technique. I will consult external resources to confirm the definition and usage of each item. If an item is misclassified or not directly related to CoT prompting, I will flag it for removal or modification.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "I will conduct a search for recent surveys and research papers on Chain-of-Thought prompting to identify CoT variants mentioned in these sources that are not present in the provided list and add any missing variants to the list.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "I have compared all pairs of items in the list and identified potential duplicates or very similar techniques. For example, 'Graph CoT' and 'Graph-of-Thoughts (GoT)' seem very similar, and 'CoT-Guided ToT/GoT', 'CoT for Branch Selection in ToT', and 'CoT for Node Generation in GoT' also appear closely related.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "I will search for original papers describing the mentioned techniques and compare names from the list with names used in the original papers. Then I will correct names if needed.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Identified general items: 'CoT combined with Knowledge Retrieval', 'Task-Specific CoT'.\n",
      "Researched specific techniques that fall under these categories.\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# analysis from all tasks\n",
    "sep = f'\\n{'-' * 40}\\n'\n",
    "for sc_summ in verification_output['simple_task_response']:\n",
    "    for line in sc_summ.analysis.split('\\n'):\n",
    "        print(line)\n",
    "    print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4f9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes are needed. The list appears to be comprehensive and accurate. All listed items are valid variants or applications of Chain-of-Thought (CoT) prompting techniques. Therefore, no modifications are required at this time. The answer is correct and comprehensive\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The list seems relatively comprehensive, but it could benefit from the inclusion of the following CoT variants: - Multimodal CoT: This variant incorporates multiple modalities, such as text and images, into the CoT process. - Step-Back Prompting: This involves prompting the model to take a step back and consider the bigger picture before answering a question. - Generated Knowledge Prompting: This technique encourages the model to generate relevant knowledge before answering a question. Adding these techniques would make the list more complete and up-to-date. Also consider merging similar techniques, such as Graph CoT and Graph-of-Thoughts (GoT), and explicitly mentioning their relationship or differences to avoid redundancy and improve clarity. Also Program-of-Thoughts (PoT) can be considered a sub-variant of ToT, GoT, where the 'thoughts' are programs. Consider adding a small description to each item to clarify the differences between them\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Based on the analysis, I will provide specific recommendations for correcting any inaccurate names or terminology used in the list, citing the appropriate sources for verification. The recommendations will include the exact text to add, remove, or modify to align with the standard terminology used in research papers and surveys.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Consider removing 'CoT combined with Knowledge Retrieval' from the list as it represents a combination of CoT with another method, rather than a distinct CoT variant itself. It might be better placed in a section discussing CoT's integration with other techniques, if such a section exists or is planned. Alternatively, rename it to Knowledge Retrieval Augmented CoT or similar to clarify its nature as a combined approach if kept in the list.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The list should be reorganized into the following categories:\n",
      "\n",
      "**1. Foundational CoT:**\n",
      "*   Standard CoT\n",
      "*   Zero-shot CoT\n",
      "\n",
      "**2. Reasoning Refinement Techniques:**\n",
      "*   Self-Consistency\n",
      "*   Finetuned CoT\n",
      "*   Least-to-Most prompting\n",
      "*   Automatic Prompt Optimization (APO)\n",
      "*   CoT with Verification\n",
      "*   Self-Refinement\n",
      "*   CoT with External Verification\n",
      "*   Constraint-Based CoT\n",
      "*   Complexity-Based Prompting\n",
      "*   Task-Specific CoT\n",
      "\n",
      "**3. Multi-Path Exploration Techniques:**\n",
      "*   Graph CoT\n",
      "*   Tree-of-Thoughts (ToT)\n",
      "*   Graph-of-Thoughts (GoT)\n",
      "*   Program-of-Thoughts (PoT)\n",
      "*   CoT-Guided ToT/GoT\n",
      "*   CoT for Branch Selection in ToT\n",
      "*   CoT for Node Generation in GoT\n",
      "\n",
      "**4. Knowledge Integration Techniques:**\n",
      "*   CoT combined with Knowledge Retrieval\n",
      "\n",
      "**5. Retrospective Refinement Techniques:**\n",
      "*   Chain-of-Hindsight (CoH)\n",
      "\n",
      "**6. Model Compression Techniques:**\n",
      "*   Distillation for CoT\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# separate recommendations from all tasks\n",
    "sep = f'\\n{'-' * 40}\\n'\n",
    "for sc_summ in verification_output['simple_task_response']:\n",
    "    for line in sc_summ.recommendations.split('\\n'):\n",
    "        print(line)\n",
    "    print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3791473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variants of Chain-of-Thought (CoT) prompting techniques are:\n",
      "\n",
      "*   Standard CoT\n",
      "*   Zero-shot CoT\n",
      "*   Self-Consistency\n",
      "*   Finetuned CoT\n",
      "*   Least-to-Most prompting\n",
      "*   Automatic Prompt Optimization (APO)\n",
      "*   CoT with Verification\n",
      "*   Graph CoT\n",
      "*   Tree-of-Thoughts (ToT)\n",
      "*   Distillation for CoT\n",
      "*   Complexity-Based Prompting\n",
      "*   Task-Specific CoT\n",
      "*   Self-Refinement\n",
      "*   CoT with External Verification\n",
      "*   Constraint-Based CoT\n",
      "*   Graph-of-Thoughts (GoT)\n",
      "*   Program-of-Thoughts (PoT)\n",
      "*   Chain-of-Hindsight (CoH)\n",
      "*   CoT-Guided ToT/GoT\n",
      "*   CoT for Branch Selection in ToT\n",
      "*   CoT for Node Generation in GoT\n",
      "*   Multimodal CoT\n",
      "*   Step-Back Prompting\n",
      "*   Generated Knowledge Prompting\n",
      "*   Knowledge Retrieval Augmented CoT\n"
     ]
    }
   ],
   "source": [
    "for line in verification_output['verified_answer'].answer.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f5fc1",
   "metadata": {},
   "source": [
    "I want to create a prompt for a research verification tasks distribution. I have a problem that besides of implicitly forbidden external sources usage in analysis methodology I still receive points like check in research papers, research, check in the literature, check in the sources, etc. I want to avoid this and I want to create a prompt that will not allow this.\n",
    "\n",
    "PROMPT:\n",
    "------------------\n",
    "You play a role of Fact Checking Manager who needs to verify received answer accordingly to given task and its context.\n",
    "You have to dirtribute different verification jobs to check the correctness of the answer.\n",
    "\n",
    "-----\n",
    "TASK:\n",
    "{task}\n",
    "-----\n",
    "CONTEXT:\n",
    "{context}\n",
    "-----\n",
    "ANSWER:\n",
    "{answer}\n",
    "-----\n",
    "\n",
    "Analysis:\n",
    "Firstly conduct analysis of provided items in following sections:\n",
    "1. Decompose and interpret task's content to retreive:\n",
    "    - main subject of the task\n",
    "    - answer format (e.g. list, list with descriptions, descriptive, descriptive with bullet points, true/false, short information, undefined)\n",
    "2. Use context content augmented with task to retreive:\n",
    "    - list of restrictions for an answer (e.g. number of elements, answer length, things to avoid etc.)\n",
    "    - list of additional information helpful for answer generation\n",
    "3. Conduct basic answer analysis to formulate key doubts based on outputs for points 1 & 2.\n",
    "    - adjust your analysis to individual shape of the task and the answer\n",
    "    - write some thesis about what can be wrong with this particular answer\n",
    "\n",
    "For clarity separate each point with a line break.\n",
    "\n",
    "Prompts:\n",
    "Formulate from {min_ver_prompts} to {max_ver_prompts} job PROMPTS to prepare recommendations for answer correction.\n",
    "Every prompt should follow PROMPT FORMAT. You can use some JOBS TEMPLATES for formulate prompts but also you can develop some on your own.\n",
    "Try to cover all your concerns from analysis. If they potentially exceed {max_ver_prompts} prompts limit you can group them.\n",
    "When describing job be very specific, take into account individual aspects from analysis.\n",
    "Task, context and answer contents will be passed to LLM by system message so they shouldn't be included in prompt (except some parts highlights) but you can refer to them.\n",
    "\n",
    "!WARNING:\n",
    "**Do not include external sources usage in analysis methodology description. All jobs should be performed by pure LLM.**\n",
    "\n",
    "-----\n",
    "PROMPT FORMAT:\n",
    "Job: describe the verification goal\n",
    "Dobuts: list potential problems from your analysis that should be solved\n",
    "Additional Info: add anything else that could be helpful to solve the job, e.g. fragment of an answer, task, context or analysis to focus on.\n",
    "Analysis methodology: Step by step detailed instruction for analysis to be performed to solve the particular job. Write every step as a numerated item.\n",
    "\n",
    "-----\n",
    "JOBS TEMPLATES:\n",
    "Here are some descriptions of jobs that can be delegated with hints of when to use them included in [...].\n",
    "Templates are just gerenal guides on what to focus on, they should be adjusted to the specific problem.\n",
    "\n",
    "-[list items] Verification that every single element of the list should be there\n",
    "-[list items] Check if elements are properly grouped if they are or should be\n",
    "-[list] Check if list contain all items it should\n",
    "-[answer length] Check if answer length is correct if it is specified\n",
    "-[answer format] Verify how answer format relates to given task specification\n",
    "-[facts] Assure if particular information is correct\n",
    "-[task reflection] Evaluate if the answer really reflect the given task\n",
    "\n",
    "-----\n",
    "\n",
    "Return separate fields for the Analysis and Prompts parts.\n",
    "------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
