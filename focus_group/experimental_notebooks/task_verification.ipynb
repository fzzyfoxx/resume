{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e927141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.chatbots.chatbot import BaseChatBot\n",
    "from typing import Dict, Any, List, Annotated\n",
    "from pydantic import BaseModel\n",
    "from langgraph.constants import Send\n",
    "from operator import add\n",
    "from fcgb.types.research import SimpleTaskModel\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5b87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTaskDistributionChatBot(BaseChatBot):\n",
    "    \"\"\"\n",
    "    A chatbot that verifies the output of a task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            llm,\n",
    "            initial_messages_spec,\n",
    "            internal_messages_spec,\n",
    "            memory,\n",
    "            global_inputs={},\n",
    "            init_values={},\n",
    "            prompt_manager_spec = {}\n",
    "        ):\n",
    "\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            initial_messages_spec=initial_messages_spec,\n",
    "            internal_messages_spec=internal_messages_spec,\n",
    "            memory=memory,\n",
    "            init_values=init_values,\n",
    "            prompt_manager_spec=prompt_manager_spec,\n",
    "            global_inputs=global_inputs,\n",
    "            compile=False\n",
    "            )\n",
    "\n",
    "        self.compile_graph()\n",
    "\n",
    "    def _set_state_class(self):\n",
    "\n",
    "        task_list_model = self.internal_messages_spec['task_list']['answer_format']\n",
    "        task_answer_model = self.internal_messages_spec['task']['answer_format']\n",
    "        answer_model = self.internal_messages_spec['answer']['answer_format']\n",
    "\n",
    "        class VerificationState(BaseModel):\n",
    "            parent_thread_id: str \n",
    "            template_inputs: Dict[str, str]\n",
    "            task_list: task_list_model | None # type: ignore\n",
    "            simple_task_response: Annotated[List[task_answer_model], add] # type: ignore\n",
    "            verified_answer: answer_model | None # type: ignore\n",
    "\n",
    "        self.state_class = VerificationState\n",
    "\n",
    "    def _set_tasks_gen_func(self):\n",
    "        \"\"\"\n",
    "        Set the function that generates the verification tasks.\n",
    "        \"\"\"\n",
    "        \n",
    "        def tasks_gen_func(state: self.state_class) -> Dict: # type: ignore\n",
    "            \n",
    "            task_list = self._invoke_internal_msg(name='task_list', template_inputs=state.template_inputs)\n",
    "\n",
    "            return {'task_list': task_list}\n",
    "        \n",
    "        return tasks_gen_func\n",
    "        \n",
    "    def _set_tasks_router_func(self):\n",
    "        \"\"\"\n",
    "        Set the function that distributes tasks along nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        def tasks_router_func(state: self.state_class) -> SimpleTaskModel: # type: ignore\n",
    "\n",
    "            return [Send('task_node',\n",
    "                        {\n",
    "                        'template_inputs': state.template_inputs,\n",
    "                        'task': task,\n",
    "                        'simple_task_response': None\n",
    "                        }) for task in state.task_list.tasks]\n",
    "        \n",
    "        return tasks_router_func\n",
    "    \n",
    "    def _set_task_node_func(self):\n",
    "        \"\"\"\n",
    "        Set the function that solve the tasks.\n",
    "        \"\"\"\n",
    "        \n",
    "        def task_node_func(state: SimpleTaskModel) -> Dict:\n",
    "\n",
    "            system = self.internal_prompts['task']['prompt'].format(**state['template_inputs'], **self.global_inputs)\n",
    "            prompt = state['task'].format(**state['template_inputs'], **self.global_inputs)\n",
    "\n",
    "            messages = [SystemMessage(content=system), HumanMessage(content=prompt)]\n",
    "            task_response = self.llm.with_structured_output(self.internal_prompts['task']['answer_format']).invoke(messages)\n",
    "\n",
    "            return {'simple_task_response': [task_response]}\n",
    "        \n",
    "        return task_node_func\n",
    "    \n",
    "    def _set_output_node_func(self):\n",
    "\n",
    "        def output_node_func(state: self.state_class) -> Dict: # type: ignore\n",
    "\n",
    "            task_response = '\\n'.join([resp.recommendations for resp in state.simple_task_response])\n",
    "\n",
    "            output = self._invoke_internal_msg(name='answer', template_inputs=state.template_inputs | {'task_response': task_response})\n",
    "\n",
    "            return {'verified_answer': output}\n",
    "        \n",
    "        return output_node_func\n",
    "    \n",
    "    def _compile_graph(self):\n",
    "\n",
    "        task_gen_func = self._set_tasks_gen_func()\n",
    "        tasks_router_func = self._set_tasks_router_func()\n",
    "        task_node_func = self._set_task_node_func()\n",
    "        output_node_func = self._set_output_node_func()\n",
    "\n",
    "        workflow = StateGraph(self.state_class)\n",
    "        workflow.add_node('task_gen', task_gen_func)\n",
    "        workflow.add_node('task_node', task_node_func)\n",
    "        workflow.add_node('output_node', output_node_func)\n",
    "\n",
    "        workflow.add_edge(START, 'task_gen')\n",
    "        workflow.add_conditional_edges('task_gen', tasks_router_func, ['task_node'])\n",
    "        workflow.add_edge('task_node', 'output_node')\n",
    "        workflow.add_edge('output_node', END)\n",
    "\n",
    "        self.graph = workflow.compile(\n",
    "            checkpointer=self.memory,\n",
    "        )\n",
    "\n",
    "    def run(self, template_inputs: Dict[str, str], thread_id: str, parent_thread_id: str):\n",
    "\n",
    "        self.init_thread(thread_id=thread_id, template_inputs=template_inputs)\n",
    "\n",
    "        config = self._get_config(thread_id)\n",
    "\n",
    "        return self.graph.invoke({'parent_thread_id': parent_thread_id, 'template_inputs': template_inputs}, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3d84b",
   "metadata": {},
   "source": [
    "## fake llm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_research_inputs = {\n",
    "    'template_inputs': {\n",
    "        'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
    "        'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.',\n",
    "    },\n",
    "    'ssc_thread_id': 'test_2/ssc1',\n",
    "    'parent_thread_id': 'test_2'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b72021fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.chatbots.specbots import SelfConvForStrategySpecBot, StrategizedSelfResearchSpecBot\n",
    "from fcgb.fake_models import FakeLLM\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "fake_llm = FakeLLM()\n",
    "\n",
    "memory = MemorySaver()\n",
    "convbot = SelfConvForStrategySpecBot(llm=fake_llm, memory=memory)\n",
    "strategybot = StrategizedSelfResearchSpecBot(llm=fake_llm, self_conv_bot=convbot, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d48e9dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting.',\n",
       "  'context': 'You want to prepare a full list of different CoT prompting methods for further research.'},\n",
       " 'ssc_thread_id': 'test_2/ssc1',\n",
       " 'parent_thread_id': 'test_2',\n",
       " 'ssc_summary': SimpleAnswerModel(answer='Fake string gybaw'),\n",
       " 'strategies': StrategyTaskModel(strategies=[{'strategy_description': 'Fake string ajvzj', 'paraphrased_task': 'Fake string hidjp', 'paraphrased_context': 'Fake string kcrsz'}, {'strategy_description': 'Fake string acpzv', 'paraphrased_task': 'Fake string fesqu', 'paraphrased_context': 'Fake string szngl'}, {'strategy_description': 'Fake string hifza', 'paraphrased_task': 'Fake string tewjq', 'paraphrased_context': 'Fake string wxjlx'}]),\n",
       " 'sc_thread_id': ['test_2/ssc1/self_conv0',\n",
       "  'test_2/ssc1/self_conv1',\n",
       "  'test_2/ssc1/self_conv2'],\n",
       " 'sc_summary': ['Fake string dijpp', 'Fake string ivagf', 'Fake string bzvwi']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_output = strategybot.run(**self_research_inputs)\n",
    "research_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07af7f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_inputs ={\n",
    "    'template_inputs': self_research_inputs['template_inputs'] | {'answer': research_output['ssc_summary'].answer},\n",
    "    'thread_id': self_research_inputs['ssc_thread_id'] + '/ver',\n",
    "    'parent_thread_id': self_research_inputs['ssc_thread_id']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c62b6e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.cfg.chat_inputs_spec import ResearchVerificationConfig\n",
    "\n",
    "verificationbot = SimpleTaskDistributionChatBot(\n",
    "    llm=fake_llm,\n",
    "    initial_messages_spec=ResearchVerificationConfig.initial_messages_spec,\n",
    "    internal_messages_spec=ResearchVerificationConfig.internal_messages_spec,\n",
    "    memory=memory,\n",
    "    global_inputs=ResearchVerificationConfig.global_inputs,\n",
    "    init_values=ResearchVerificationConfig.init_values,\n",
    "    prompt_manager_spec=ResearchVerificationConfig.prompt_manager_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7377ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parent_thread_id': 'test_2/ssc1',\n",
       " 'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting.',\n",
       "  'context': 'You want to prepare a full list of different CoT prompting methods for further research.',\n",
       "  'answer': 'Fake string gybaw'},\n",
       " 'task_list': PromptTemplatesListModel(analysis='Fake string vfksh', tasks=['Fake string jiukt', 'Fake string gvsbb']),\n",
       " 'simple_task_response': [SingleVerificationModel(analysis='Fake string gcynr', recommendations='Fake string ecmkp'),\n",
       "  SingleVerificationModel(analysis='Fake string mltxy', recommendations='Fake string tydmj')],\n",
       " 'verified_answer': SimpleAnswerModel(answer='Fake string xapnf')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verification_output = verificationbot.run(**verification_inputs)\n",
    "verification_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bee1c5",
   "metadata": {},
   "source": [
    "## real llm test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04b6c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.cfg.precompiled import get_checkpointer, get_llm\n",
    "\n",
    "memory = get_checkpointer(checkpointer_mode='mongodb', mode='test')\n",
    "llm = get_llm(llm_model='google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ade27e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcgb.chatbots.specbots import SelfConvForStrategySpecBot, StrategizedSelfResearchSpecBot\n",
    "from fcgb.cfg.chat_inputs_spec import ResearchVerificationConfig\n",
    "\n",
    "convbot = SelfConvForStrategySpecBot(llm=llm, memory=memory)\n",
    "strategybot = StrategizedSelfResearchSpecBot(llm=llm, self_conv_bot=convbot, memory=memory)\n",
    "verificationbot = SimpleTaskDistributionChatBot(\n",
    "    llm=llm,\n",
    "    initial_messages_spec=ResearchVerificationConfig.initial_messages_spec,\n",
    "    internal_messages_spec=ResearchVerificationConfig.internal_messages_spec,\n",
    "    memory=memory,\n",
    "    global_inputs=ResearchVerificationConfig.global_inputs,\n",
    "    init_values=ResearchVerificationConfig.init_values,\n",
    "    prompt_manager_spec=ResearchVerificationConfig.prompt_manager_spec\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72db1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from fcgb.cfg.precompiled import get_db_client\n",
    "\n",
    "db = get_db_client(db_engine='mongodb', mode='test')\n",
    "\n",
    "db['checkpoints_writes'].delete_many({'thread_id': 'partial/research/cot/ssc1'})\n",
    "db['checkpoints'].delete_many({'thread_id': 'partial/research/cot/ssc1'})\n",
    "\n",
    "strategybot.get_state('partial/research/cot/ssc1')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fac4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_research_inputs = {\n",
    "    'template_inputs': {\n",
    "        'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
    "        'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.',\n",
    "    },\n",
    "    'ssc_thread_id': 'partial/research/cot/ssc4',\n",
    "    'parent_thread_id': 'partial/research/cot'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c2aa08",
   "metadata": {},
   "source": [
    "For a task which is: 'List all variants of Chain-of-Thought (CoT) prompting techniques.'\n",
    "And a context for it: 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.'\n",
    "\n",
    "List possible mistakes that LLM can make answering for a given task and context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19bac65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
       "  'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.'},\n",
       " 'ssc_thread_id': 'partial/research/cot/ssc4',\n",
       " 'parent_thread_id': 'partial/research/cot',\n",
       " 'ssc_summary': SimpleAnswerModel(answer='The variants of Chain-of-Thought (CoT) prompting techniques are:\\n\\n*   Standard CoT\\n*   Few-shot CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Tree-of-Thoughts (ToT)\\n*   Graph-of-Thoughts (GoT)\\n*   Program-aided CoT / Program-of-Thoughts (PoT)\\n*   CoT with Knowledge Graphs\\n*   Active Prompting with CoT / Active Prompting with Few-shot CoT\\n*   Multimodal CoT\\n*   Self-Consistency with Tree-of-Thoughts\\n*   Program-aided CoT with Knowledge Graphs\\n*   Least-to-Most CoT\\n*   Chain-of-Verification (CoV)\\n*   Step-Back Prompting\\n*   Generated Knowledge Prompting'),\n",
       " 'strategies': StrategyTaskModel(strategies=[{'strategy_description': 'Comprehensive Coverage: Focus on breadth. The researcher should aim to identify and list as many CoT variants as possible, even those that are less common or highly specialized. Prompt the expert to consider novel or hybrid approaches.', 'paraphrased_task': 'What are the different ways to use Chain-of-Thought prompting?', 'paraphrased_context': \"I'm writing a guide for LLM users about Chain-of-Thought prompting. Help me explain CoT techniques effectively.\"}, {'strategy_description': 'Detailed Explanation: Emphasize depth and clarity. The researcher should encourage the expert to elaborate on the nuances of each CoT technique, including examples and potential pitfalls. Ask probing questions about the underlying mechanisms and practical considerations.', 'paraphrased_task': 'Can you provide a detailed breakdown of the main Chain-of-Thought prompting techniques?', 'paraphrased_context': \"I'm creating a resource for people learning about LLMs, specifically Chain-of-Thought prompting. I want to make sure I cover all the important CoT methods.\"}, {'strategy_description': 'Practical Application: The researcher should steer the conversation toward real-world applications and implementation details of each CoT technique. The goal is to gather information that is immediately useful for readers who want to apply CoT in their own projects.', 'paraphrased_task': 'Please list and describe Chain-of-Thought prompting techniques, and focus on the practical usage of each of them.', 'paraphrased_context': 'I am writing a chapter about CoT prompting techniques for LLM users.'}]),\n",
       " 'sc_thread_id': ['partial/research/cot/ssc4/self_conv0',\n",
       "  'partial/research/cot/ssc4/self_conv1',\n",
       "  'partial/research/cot/ssc4/self_conv2'],\n",
       " 'sc_summary': [\"Here's a list of Chain-of-Thought (CoT) prompting techniques:\\n\\n*   Standard CoT\\n*   Few-shot CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Tree-of-Thoughts (ToT)\\n*   Graph-of-Thoughts (GoT)\\n*   Program-aided CoT\\n*   CoT with Knowledge Graphs\\n*   Active Prompting with CoT\\n*   Multimodal CoT\\n*   Self-Consistency with Tree-of-Thoughts\\n*   Program-aided CoT with Knowledge Graphs\\n*   Active Prompting with Few-shot CoT\",\n",
       "  'The main Chain-of-Thought (CoT) prompting techniques are:\\n\\n1. Standard CoT\\n2. Zero-shot CoT\\n3. Self-Consistency CoT\\n4. Least-to-Most CoT\\n5. Tree-of-Thoughts (ToT)\\n6. Graph of Thoughts (GoT).',\n",
       "  'Chain-of-Thought (CoT) prompting techniques:\\n\\n*   Standard CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Tree-of-Thoughts (ToT)\\n*   Graph of Thoughts (GoT)\\n*   Program-of-Thoughts (PoT)\\n*   Chain-of-Verification (CoV)\\n*   Step-Back Prompting\\n*   Generated Knowledge Prompting']}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "research_output = strategybot.run(**self_research_inputs)\n",
    "research_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "835c799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_output = strategybot.get_state(self_research_inputs['ssc_thread_id'])\n",
    "research_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a420c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The variants of Chain-of-Thought (CoT) prompting techniques are:\n",
      "\n",
      "*   Standard CoT\n",
      "*   Few-shot CoT\n",
      "*   Zero-shot CoT\n",
      "*   Self-Consistency\n",
      "*   Tree-of-Thoughts (ToT)\n",
      "*   Graph-of-Thoughts (GoT)\n",
      "*   Program-aided CoT / Program-of-Thoughts (PoT)\n",
      "*   CoT with Knowledge Graphs\n",
      "*   Active Prompting with CoT / Active Prompting with Few-shot CoT\n",
      "*   Multimodal CoT\n",
      "*   Self-Consistency with Tree-of-Thoughts\n",
      "*   Program-aided CoT with Knowledge Graphs\n",
      "*   Least-to-Most CoT\n",
      "*   Chain-of-Verification (CoV)\n",
      "*   Step-Back Prompting\n",
      "*   Generated Knowledge Prompting\n"
     ]
    }
   ],
   "source": [
    "for line in research_output['ssc_summary'].answer.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea647bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_inputs ={\n",
    "    'template_inputs': self_research_inputs['template_inputs'] | {'answer': research_output['ssc_summary'].answer},\n",
    "    'thread_id': self_research_inputs['ssc_thread_id'] + '/ver1',\n",
    "    'parent_thread_id': self_research_inputs['ssc_thread_id']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ca85b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'parent_thread_id': 'partial/research/cot/ssc4',\n",
       " 'template_inputs': {'task': 'List all variants of Chain-of-Thought (CoT) prompting techniques.',\n",
       "  'context': 'CoT prompting techniques list is necessary to plan a chapter about CoT prompting. It is dedicated to potential users of LLMs who want to learn how to efficiently use LLMs.',\n",
       "  'answer': 'The variants of Chain-of-Thought (CoT) prompting techniques are:\\n\\n*   Standard CoT\\n*   Few-shot CoT\\n*   Zero-shot CoT\\n*   Self-Consistency\\n*   Tree-of-Thoughts (ToT)\\n*   Graph-of-Thoughts (GoT)\\n*   Program-aided CoT / Program-of-Thoughts (PoT)\\n*   CoT with Knowledge Graphs\\n*   Active Prompting with CoT / Active Prompting with Few-shot CoT\\n*   Multimodal CoT\\n*   Self-Consistency with Tree-of-Thoughts\\n*   Program-aided CoT with Knowledge Graphs\\n*   Least-to-Most CoT\\n*   Chain-of-Verification (CoV)\\n*   Step-Back Prompting\\n*   Generated Knowledge Prompting'},\n",
       " 'task_list': PromptTemplatesListModel(analysis='The answer provides a list of Chain-of-Thought (CoT) prompting techniques. The task is to list all variants, and the context specifies that this list is for potential LLM users learning to use LLMs efficiently. Therefore, the list should be comprehensive, accurate, and relevant to practical LLM usage. Potential mistakes include: 1) Inaccuracy: Listing techniques that are not actual CoT variants or are misnamed. 2) Redundancy: Listing essentially the same technique under different names. 3) Irrelevance: Including highly theoretical or impractical techniques that are not useful for typical LLM users. 4) Omission: Missing important and widely used CoT variants. 5) Lack of clarity: Not providing enough information to distinguish between similar techniques. 6) Hallucination: Inventing CoT variants that do not exist. 7) Overly specific/niche techniques: Including variants that are only applicable in very narrow scenarios, which might not be helpful for the target audience. 8) Incorrect grouping: Misclassifying a technique as a CoT variant when it belongs to a different category of prompting techniques. 9) Outdated information: Including techniques that are no longer considered relevant or have been superseded by newer methods.', tasks=['Verify the accuracy of each listed technique. Is it a real CoT variant?', 'Check for redundancy. Are any techniques listed multiple times under different names?', 'Assess the relevance of each technique to practical LLM usage. Is it useful for typical users?', 'Identify any omissions. Are there important CoT variants missing from the list?', 'Evaluate the clarity of the descriptions. Can a user easily understand the difference between similar techniques?', 'Check for hallucinations. Are any techniques invented or non-existent?', 'Determine if any techniques are overly specific or niche, making them less useful for the target audience.', 'Verify the correct grouping of techniques. Are all listed items actually CoT variants?', 'Assess the timeliness of the information. Are any techniques outdated or superseded?']),\n",
       " 'simple_task_response': [SingleVerificationModel(analysis='All listed items appear to be valid variants or extensions of the Chain-of-Thought (CoT) prompting technique. They represent different approaches to improve reasoning and problem-solving in large language models.', recommendations='No changes are needed. The list seems comprehensive and accurate based on current research and literature on CoT prompting techniques. Each item represents a distinct variation or extension of the original CoT approach, addressing different aspects of reasoning or incorporating external knowledge sources or modalities..'),\n",
       "  SingleVerificationModel(analysis=\"The list seems comprehensive and covers a wide range of CoT variants. However, some entries might be redundant or represent combinations of other techniques. Need to verify if 'Self-Consistency with Tree-of-Thoughts' is simply a combination of 'Self-Consistency' and 'Tree-of-Thoughts'. Similarly, 'Program-aided CoT with Knowledge Graphs' combines 'Program-aided CoT' and 'CoT with Knowledge Graphs'. Also, 'Program-aided CoT' and 'Program-of-Thoughts (PoT)' seem to refer to the same technique.\", recommendations=\"1.  Consider removing 'Self-Consistency with Tree-of-Thoughts' if it's adequately covered by discussing 'Self-Consistency' and 'Tree-of-Thoughts' separately.\\n2.  Consider removing 'Program-aided CoT with Knowledge Graphs' if it's adequately covered by discussing 'Program-aided CoT' and 'CoT with Knowledge Graphs' separately.\\n3.  Clarify that 'Program-aided CoT' and 'Program-of-Thoughts (PoT)' refer to the same technique and potentially merge them into a single entry.\\n\"),\n",
       "  SingleVerificationModel(analysis='The provided list of Chain-of-Thought (CoT) prompting techniques is comprehensive and includes many relevant variants. The list covers a wide spectrum of CoT approaches, from basic ones like Standard CoT and Few-shot CoT to more advanced techniques like Tree-of-Thoughts and Graph-of-Thoughts. It also includes specialized variants like Program-aided CoT, CoT with Knowledge Graphs, and Multimodal CoT. Techniques like Active Prompting, Least-to-Most CoT, Chain-of-Verification, Step-Back Prompting, and Generated Knowledge Prompting are also included, increasing the completeness of the list. The relevance of each technique to practical LLM usage varies, with some being more accessible and useful for typical users than others. Overall, the list provides a good overview of the CoT landscape.', recommendations=\"To enhance the answer's usefulness for potential LLM users, I recommend adding a brief note about the practical applicability and complexity of each technique. For example, indicating which techniques are easier to implement and more suitable for beginners, and which are more advanced and require specific knowledge or resources. This would help users prioritize their learning and experimentation efforts. Also, re-organize the list in terms of complexity, from most basic to most advanced. This will help users to progressively learn about CoT techniques. Finally, it would be useful to add a short description of each technique to clarify its purpose and how it differs from the others. This would make the list more self-contained and informative for readers.\"),\n",
       "  SingleVerificationModel(analysis=\"The provided list seems comprehensive, covering many prominent CoT variants. However, a few potential omissions could be considered. One is 'Backward CoT'. Also, 'Chain of Hindsight' could be added. Finally, 'Structured Chain of Thought' could be considered.\", recommendations=\"Add 'Backward CoT', 'Chain of Hindsight', and 'Structured Chain of Thought' to the list of CoT variants for increased completeness. Also, consider merging similar techniques or adding brief descriptions to differentiate them if the target audience is new to the field. For example, Program-aided CoT / Program-of-Thoughts (PoT) could be clarified as using external tools or code execution to aid reasoning. Similarly, CoT with Knowledge Graphs could be described as incorporating structured knowledge from KGs into the reasoning process. This would enhance clarity and educational value for potential users of LLMs.\"),\n",
       "  SingleVerificationModel(analysis=\"The list provides a comprehensive overview of various CoT prompting techniques. However, it lacks descriptions that would allow a user to easily understand the nuances and differences between similar techniques. For example, the distinction between Tree-of-Thoughts and Graph-of-Thoughts might not be immediately clear to a novice user. Similarly, the differences between Program-aided CoT and CoT with Knowledge Graphs could be better explained. The terms 'Standard CoT' and 'Few-shot CoT' are somewhat redundant as Standard CoT is generally understood to be Few-shot CoT. Zero-shot CoT is distinct and should be retained. The combinations of techniques (e.g., Self-Consistency with Tree-of-Thoughts, Program-aided CoT with Knowledge Graphs) are useful to include, but their descriptions are essential.\", recommendations=\"Add brief descriptions for each technique to clarify their purpose and how they differ from one another. Consider merging 'Standard CoT' and 'Few-shot CoT' or clarifying their relationship. Provide more context for combined techniques to explain their added value. For techniques like Program-aided CoT, explicitly mention what programming language or tool is used. For Knowledge Graphs, specify the type of knowledge being integrated and how it enhances reasoning. Consider re-organizing the list into categories based on the core CoT method and extensions/combinations for better readability. \"),\n",
       "  SingleVerificationModel(analysis='The list seems comprehensive and covers many known CoT variants. All listed techniques appear to be valid and actively researched or used in the field of LLMs. There are no immediately obvious hallucinations or invented techniques.', recommendations=\"No changes are needed as the list appears accurate and complete. However, for increased clarity for potential LLM users, a brief explanation of each technique could be added in the chapter itself, but this is beyond the scope of the current verification task which is focused on identifying hallucinations or non-existent techniques. Consider adding a brief description of each technique in the main text where this list will be used, for example, 'Standard CoT: ...explanation...'\"),\n",
       "  SingleVerificationModel(analysis=\"The answer provides a comprehensive list of Chain-of-Thought (CoT) prompting techniques. Some techniques like 'CoT with Knowledge Graphs,' 'Program-aided CoT with Knowledge Graphs,' and 'Self-Consistency with Tree-of-Thoughts' are more specific and might be considered niche for a general audience learning about CoT. Including them is not incorrect, but it's important to consider the level of detail appropriate for the intended audience. Other techniques such as 'Standard CoT', 'Few-shot CoT', 'Zero-shot CoT', 'Self-Consistency', 'Tree-of-Thoughts (ToT)', 'Least-to-Most CoT', 'Chain-of-Verification (CoV)', 'Step-Back Prompting', 'Generated Knowledge Prompting' are relevant and helpful for beginners.\", recommendations=\"I recommend grouping the more specific techniques (e.g., 'CoT with Knowledge Graphs,' 'Program-aided CoT with Knowledge Graphs,' and 'Self-Consistency with Tree-of-Thoughts') under a broader category like 'Advanced CoT Techniques' or 'Specialized CoT Variants'. This would keep the initial list more accessible while still providing a pathway for users who want to explore more complex applications. Consider adding a brief description of each technique to clarify its purpose and use case, especially for the less common ones. Consider re-ordering the list to put the basic and most common CoT techniques first, to make it easier for a beginner to grasp the fundamentals before diving into more advanced topics. \"),\n",
       "  SingleVerificationModel(analysis=\"The provided list appears to be a comprehensive collection of Chain-of-Thought (CoT) prompting techniques and their variants. Each item represents a distinct approach or extension of the original CoT prompting method. The grouping seems correct, as all listed techniques build upon or modify the core idea of generating intermediate reasoning steps. However, a deeper dive into each technique might reveal subtle nuances in their classification. It's also important to consider that the field is rapidly evolving, and new variants or combinations may emerge.\", recommendations='No specific changes are needed at this time. The list seems accurate and relevant. However, consider adding brief descriptions of each technique to enhance clarity for potential users. Also, it may be helpful to categorize the techniques based on their primary focus (e.g., knowledge integration, multi-modality, active learning) for better organization. Finally, keep the list updated as new CoT variants are introduced in the research community. '),\n",
       "  SingleVerificationModel(analysis='The list appears to be comprehensive and includes both fundamental and more recent CoT variants. There is no indication of outdated or superseded techniques. All listed techniques are still relevant in the field of LLM prompting.', recommendations='No changes are needed regarding the timeliness of the information. The list is up-to-date as far as I can verify.')],\n",
       " 'verified_answer': SimpleAnswerModel(answer='Chain-of-Thought (CoT) Prompting Techniques: A Beginner\\'s Guide\\n\\nThis list introduces various Chain-of-Thought (CoT) prompting techniques, designed to enhance the reasoning capabilities of Large Language Models (LLMs). We\\'ve organized them from basic to advanced to facilitate progressive learning.\\n\\nBasic CoT Techniques:\\n\\n*   Zero-shot CoT: Instruct the model to think step by step without providing any examples. (e.g., \"Let\\'s think step by step.\")\\n*   Few-shot CoT: Provide a few examples of questions and their step-by-step reasoning to guide the model.\\n*   Standard CoT: This is the general application of chain of thought prompting, where you encourage the model to explain its reasoning process.\\n\\nIntermediate CoT Techniques:\\n\\n*   Active Prompting with CoT: Selectively choose examples to prompt the model, optimizing for learning and performance.\\n*   Backward CoT: Start with the answer and ask the model to reason backward to the initial question.\\n*   Chain of Hindsight: After generating an answer, prompt the model to reflect on its reasoning process and identify potential errors or improvements.\\n*   Least-to-Most CoT: Break down complex problems into simpler subproblems, solving them sequentially.\\n*   Step-Back Prompting: Encourage the model to take a step back from the immediate problem and consider the broader context before reasoning.\\n*   Self-Consistency: Generate multiple reasoning paths and select the most consistent answer.\\n*   Chain-of-Verification (CoV): After generating an initial answer, prompt the model to verify each step of its reasoning to identify potential errors.\\n*   Structured Chain of Thought: Guide the model to structure its reasoning in a specific format, such as a table or a list, to improve clarity and organization.\\n\\nAdvanced CoT Techniques:\\n\\n*   Tree-of-Thoughts (ToT): Explore multiple reasoning paths in parallel, branching out and evaluating different options.\\n*   Graph-of-Thoughts (GoT): Extends ToT by allowing more complex relationships between thoughts, forming a graph structure.\\n*   Multimodal CoT: Incorporate information from multiple modalities, such as text and images, into the reasoning process.\\n*   Generated Knowledge Prompting: Prompt the model to generate relevant knowledge before answering the question.\\n*   CoT with Knowledge Graphs: Integrate structured knowledge from knowledge graphs into the reasoning process. This enhances reasoning by providing relevant facts and relationships. The type of knowledge being integrated depends on the KG used (e.g. DBpedia, Wikidata).\\n*   Program-aided CoT / Program-of-Thoughts (PoT): Use external tools or code execution (e.g., Python) to aid reasoning. This allows the model to perform calculations, look up information, or use other tools to support its reasoning process.\\n')}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verification_output = verificationbot.run(**verification_inputs)\n",
    "verification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4cbfa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer provides a list of Chain-of-Thought (CoT) prompting techniques. The task is to list all variants, and the context specifies that this list is for potential LLM users learning to use LLMs efficiently. Therefore, the list should be comprehensive, accurate, and relevant to practical LLM usage. Potential mistakes include: 1) Inaccuracy: Listing techniques that are not actual CoT variants or are misnamed. 2) Redundancy: Listing essentially the same technique under different names. 3) Irrelevance: Including highly theoretical or impractical techniques that are not useful for typical LLM users. 4) Omission: Missing important and widely used CoT variants. 5) Lack of clarity: Not providing enough information to distinguish between similar techniques. 6) Hallucination: Inventing CoT variants that do not exist. 7) Overly specific/niche techniques: Including variants that are only applicable in very narrow scenarios, which might not be helpful for the target audience. 8) Incorrect grouping: Misclassifying a technique as a CoT variant when it belongs to a different category of prompting techniques. 9) Outdated information: Including techniques that are no longer considered relevant or have been superseded by newer methods.\n"
     ]
    }
   ],
   "source": [
    "for line in verification_output['task_list'].analysis.split('\\n'):\n",
    "    print(line, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "868972f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify the accuracy of each listed technique. Is it a real CoT variant?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Check for redundancy. Are any techniques listed multiple times under different names?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Assess the relevance of each technique to practical LLM usage. Is it useful for typical users?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Identify any omissions. Are there important CoT variants missing from the list?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Evaluate the clarity of the descriptions. Can a user easily understand the difference between similar techniques?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Check for hallucinations. Are any techniques invented or non-existent?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Determine if any techniques are overly specific or niche, making them less useful for the target audience.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Verify the correct grouping of techniques. Are all listed items actually CoT variants?\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Assess the timeliness of the information. Are any techniques outdated or superseded?\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sep = f'\\n{'-' * 40}\\n'\n",
    "for sc_summ in verification_output['task_list'].tasks:\n",
    "    for line in sc_summ.split('\\n'):\n",
    "        print(line)\n",
    "    print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "09213f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All listed items appear to be valid variants or extensions of the Chain-of-Thought (CoT) prompting technique. They represent different approaches to improve reasoning and problem-solving in large language models.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The list seems comprehensive and covers a wide range of CoT variants. However, some entries might be redundant or represent combinations of other techniques. Need to verify if 'Self-Consistency with Tree-of-Thoughts' is simply a combination of 'Self-Consistency' and 'Tree-of-Thoughts'. Similarly, 'Program-aided CoT with Knowledge Graphs' combines 'Program-aided CoT' and 'CoT with Knowledge Graphs'. Also, 'Program-aided CoT' and 'Program-of-Thoughts (PoT)' seem to refer to the same technique.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The provided list of Chain-of-Thought (CoT) prompting techniques is comprehensive and includes many relevant variants. The list covers a wide spectrum of CoT approaches, from basic ones like Standard CoT and Few-shot CoT to more advanced techniques like Tree-of-Thoughts and Graph-of-Thoughts. It also includes specialized variants like Program-aided CoT, CoT with Knowledge Graphs, and Multimodal CoT. Techniques like Active Prompting, Least-to-Most CoT, Chain-of-Verification, Step-Back Prompting, and Generated Knowledge Prompting are also included, increasing the completeness of the list. The relevance of each technique to practical LLM usage varies, with some being more accessible and useful for typical users than others. Overall, the list provides a good overview of the CoT landscape.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The provided list seems comprehensive, covering many prominent CoT variants. However, a few potential omissions could be considered. One is 'Backward CoT'. Also, 'Chain of Hindsight' could be added. Finally, 'Structured Chain of Thought' could be considered.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The list provides a comprehensive overview of various CoT prompting techniques. However, it lacks descriptions that would allow a user to easily understand the nuances and differences between similar techniques. For example, the distinction between Tree-of-Thoughts and Graph-of-Thoughts might not be immediately clear to a novice user. Similarly, the differences between Program-aided CoT and CoT with Knowledge Graphs could be better explained. The terms 'Standard CoT' and 'Few-shot CoT' are somewhat redundant as Standard CoT is generally understood to be Few-shot CoT. Zero-shot CoT is distinct and should be retained. The combinations of techniques (e.g., Self-Consistency with Tree-of-Thoughts, Program-aided CoT with Knowledge Graphs) are useful to include, but their descriptions are essential.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The list seems comprehensive and covers many known CoT variants. All listed techniques appear to be valid and actively researched or used in the field of LLMs. There are no immediately obvious hallucinations or invented techniques.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The answer provides a comprehensive list of Chain-of-Thought (CoT) prompting techniques. Some techniques like 'CoT with Knowledge Graphs,' 'Program-aided CoT with Knowledge Graphs,' and 'Self-Consistency with Tree-of-Thoughts' are more specific and might be considered niche for a general audience learning about CoT. Including them is not incorrect, but it's important to consider the level of detail appropriate for the intended audience. Other techniques such as 'Standard CoT', 'Few-shot CoT', 'Zero-shot CoT', 'Self-Consistency', 'Tree-of-Thoughts (ToT)', 'Least-to-Most CoT', 'Chain-of-Verification (CoV)', 'Step-Back Prompting', 'Generated Knowledge Prompting' are relevant and helpful for beginners.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The provided list appears to be a comprehensive collection of Chain-of-Thought (CoT) prompting techniques and their variants. Each item represents a distinct approach or extension of the original CoT prompting method. The grouping seems correct, as all listed techniques build upon or modify the core idea of generating intermediate reasoning steps. However, a deeper dive into each technique might reveal subtle nuances in their classification. It's also important to consider that the field is rapidly evolving, and new variants or combinations may emerge.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "The list appears to be comprehensive and includes both fundamental and more recent CoT variants. There is no indication of outdated or superseded techniques. All listed techniques are still relevant in the field of LLM prompting.\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sep = f'\\n{'-' * 40}\\n'\n",
    "for sc_summ in verification_output['simple_task_response']:\n",
    "    for line in sc_summ.analysis.split('\\n'):\n",
    "        print(line)\n",
    "    print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e4f9958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No changes are needed. The list seems comprehensive and accurate based on current research and literature on CoT prompting techniques. Each item represents a distinct variation or extension of the original CoT approach, addressing different aspects of reasoning or incorporating external knowledge sources or modalities..\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "1.  Consider removing 'Self-Consistency with Tree-of-Thoughts' if it's adequately covered by discussing 'Self-Consistency' and 'Tree-of-Thoughts' separately.\n",
      "2.  Consider removing 'Program-aided CoT with Knowledge Graphs' if it's adequately covered by discussing 'Program-aided CoT' and 'CoT with Knowledge Graphs' separately.\n",
      "3.  Clarify that 'Program-aided CoT' and 'Program-of-Thoughts (PoT)' refer to the same technique and potentially merge them into a single entry.\n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "To enhance the answer's usefulness for potential LLM users, I recommend adding a brief note about the practical applicability and complexity of each technique. For example, indicating which techniques are easier to implement and more suitable for beginners, and which are more advanced and require specific knowledge or resources. This would help users prioritize their learning and experimentation efforts. Also, re-organize the list in terms of complexity, from most basic to most advanced. This will help users to progressively learn about CoT techniques. Finally, it would be useful to add a short description of each technique to clarify its purpose and how it differs from the others. This would make the list more self-contained and informative for readers.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Add 'Backward CoT', 'Chain of Hindsight', and 'Structured Chain of Thought' to the list of CoT variants for increased completeness. Also, consider merging similar techniques or adding brief descriptions to differentiate them if the target audience is new to the field. For example, Program-aided CoT / Program-of-Thoughts (PoT) could be clarified as using external tools or code execution to aid reasoning. Similarly, CoT with Knowledge Graphs could be described as incorporating structured knowledge from KGs into the reasoning process. This would enhance clarity and educational value for potential users of LLMs.\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Add brief descriptions for each technique to clarify their purpose and how they differ from one another. Consider merging 'Standard CoT' and 'Few-shot CoT' or clarifying their relationship. Provide more context for combined techniques to explain their added value. For techniques like Program-aided CoT, explicitly mention what programming language or tool is used. For Knowledge Graphs, specify the type of knowledge being integrated and how it enhances reasoning. Consider re-organizing the list into categories based on the core CoT method and extensions/combinations for better readability. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "No changes are needed as the list appears accurate and complete. However, for increased clarity for potential LLM users, a brief explanation of each technique could be added in the chapter itself, but this is beyond the scope of the current verification task which is focused on identifying hallucinations or non-existent techniques. Consider adding a brief description of each technique in the main text where this list will be used, for example, 'Standard CoT: ...explanation...'\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "I recommend grouping the more specific techniques (e.g., 'CoT with Knowledge Graphs,' 'Program-aided CoT with Knowledge Graphs,' and 'Self-Consistency with Tree-of-Thoughts') under a broader category like 'Advanced CoT Techniques' or 'Specialized CoT Variants'. This would keep the initial list more accessible while still providing a pathway for users who want to explore more complex applications. Consider adding a brief description of each technique to clarify its purpose and use case, especially for the less common ones. Consider re-ordering the list to put the basic and most common CoT techniques first, to make it easier for a beginner to grasp the fundamentals before diving into more advanced topics. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "No specific changes are needed at this time. The list seems accurate and relevant. However, consider adding brief descriptions of each technique to enhance clarity for potential users. Also, it may be helpful to categorize the techniques based on their primary focus (e.g., knowledge integration, multi-modality, active learning) for better organization. Finally, keep the list updated as new CoT variants are introduced in the research community. \n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "No changes are needed regarding the timeliness of the information. The list is up-to-date as far as I can verify.\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sep = f'\\n{'-' * 40}\\n'\n",
    "for sc_summ in verification_output['simple_task_response']:\n",
    "    for line in sc_summ.recommendations.split('\\n'):\n",
    "        print(line)\n",
    "    print(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e3791473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought (CoT) Prompting Techniques: A Beginner's Guide\n",
      "\n",
      "This list introduces various Chain-of-Thought (CoT) prompting techniques, designed to enhance the reasoning capabilities of Large Language Models (LLMs). We've organized them from basic to advanced to facilitate progressive learning.\n",
      "\n",
      "Basic CoT Techniques:\n",
      "\n",
      "*   Zero-shot CoT: Instruct the model to think step by step without providing any examples. (e.g., \"Let's think step by step.\")\n",
      "*   Few-shot CoT: Provide a few examples of questions and their step-by-step reasoning to guide the model.\n",
      "*   Standard CoT: This is the general application of chain of thought prompting, where you encourage the model to explain its reasoning process.\n",
      "\n",
      "Intermediate CoT Techniques:\n",
      "\n",
      "*   Active Prompting with CoT: Selectively choose examples to prompt the model, optimizing for learning and performance.\n",
      "*   Backward CoT: Start with the answer and ask the model to reason backward to the initial question.\n",
      "*   Chain of Hindsight: After generating an answer, prompt the model to reflect on its reasoning process and identify potential errors or improvements.\n",
      "*   Least-to-Most CoT: Break down complex problems into simpler subproblems, solving them sequentially.\n",
      "*   Step-Back Prompting: Encourage the model to take a step back from the immediate problem and consider the broader context before reasoning.\n",
      "*   Self-Consistency: Generate multiple reasoning paths and select the most consistent answer.\n",
      "*   Chain-of-Verification (CoV): After generating an initial answer, prompt the model to verify each step of its reasoning to identify potential errors.\n",
      "*   Structured Chain of Thought: Guide the model to structure its reasoning in a specific format, such as a table or a list, to improve clarity and organization.\n",
      "\n",
      "Advanced CoT Techniques:\n",
      "\n",
      "*   Tree-of-Thoughts (ToT): Explore multiple reasoning paths in parallel, branching out and evaluating different options.\n",
      "*   Graph-of-Thoughts (GoT): Extends ToT by allowing more complex relationships between thoughts, forming a graph structure.\n",
      "*   Multimodal CoT: Incorporate information from multiple modalities, such as text and images, into the reasoning process.\n",
      "*   Generated Knowledge Prompting: Prompt the model to generate relevant knowledge before answering the question.\n",
      "*   CoT with Knowledge Graphs: Integrate structured knowledge from knowledge graphs into the reasoning process. This enhances reasoning by providing relevant facts and relationships. The type of knowledge being integrated depends on the KG used (e.g. DBpedia, Wikidata).\n",
      "*   Program-aided CoT / Program-of-Thoughts (PoT): Use external tools or code execution (e.g., Python) to aid reasoning. This allows the model to perform calculations, look up information, or use other tools to support its reasoning process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for line in verification_output['verified_answer'].answer.split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591160f",
   "metadata": {},
   "source": [
    "For a task: 'List all variants of Chain-of-Thought (CoT) prompting.'\n",
    "Provide "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
