{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e62c1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import uuid\n",
    "from rag_eval.doc_loader import RandomQueriesPaperSearchGraph\n",
    "from rag_eval.chunk_eval import ChunkEvalGraph\n",
    "from fcgb.cfg.precompiled import get_llm, get_checkpointer\n",
    "\n",
    "import asyncio\n",
    "from tqdm.notebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e8d6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file):\n",
    "    return '.'.join(os.path.basename(file).split('.')[:-1])\n",
    "\n",
    "def get_files(path):\n",
    "    \"\"\"\n",
    "    Return list of files from a given folder.\n",
    "    \"\"\"\n",
    "    return [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "\n",
    "def get_filenames_list(path):\n",
    "    \"\"\"\n",
    "    Return list of files from a given folder without extensions.\n",
    "    \"\"\"\n",
    "    return [get_filename(f) for f in get_files(path)]\n",
    "\n",
    "def get_files_without(path, filenames):\n",
    "    \"\"\"\n",
    "    Returns a list of files from a given folder that are not in the provided filenames list.\n",
    "    \"\"\"\n",
    "    return [f for f in get_files(path) if get_filename(f) not in filenames]\n",
    "\n",
    "def get_filenames_without(path, filenames):\n",
    "    \"\"\"\n",
    "    Returns a list of files from a given folder without extensions that are not in the provided filenames list.\n",
    "    \"\"\"\n",
    "    return [get_filename(f) for f in get_files_without(path, filenames)]\n",
    "\n",
    "def remove_files(path, filenames):\n",
    "    \"\"\"\n",
    "    Remove files from a given folder that are in the provided filenames list.\n",
    "    \"\"\"\n",
    "    for f in filenames:\n",
    "        os.remove(os.path.join(path, f))\n",
    "\n",
    "def load_json(path):\n",
    "    \"\"\"\n",
    "    Load a JSON file from the given path.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9798abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated files: ['2502.09625v1_4', '2502.09625v1_0', '2502.09625v1_2', '2312.15235v1', '2502.09625v1_1', '2502.09625v1_7', '2502.09625v1_3', '2502.09625v1_5', '2502.09625v1_8', '2502.09625v1_6', '2502.09625v1_9', '2502.09625v1']\n",
      "Files to evaluate: ['2110.03478v2.json', '2206.12415v1.json', '2009.08294v1.json', '1811.08212v1.json', '2108.02501v3.json', '2406.11389v1.json', '2506.01945v1.json', '2406.05517v1.json', '2406.15962v1.json', '2306.17794v1.json', '2409.13406v1.json', '2408.12408v1.json', '2310.07427v3.json']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluated_files = get_filenames_list('../data/example_chunk_eval')\n",
    "print(f\"Evaluated files: {evaluated_files}\")\n",
    "\n",
    "files_to_evaluate = get_files_without('../data/docs_metadata', evaluated_files)\n",
    "print(f\"Files to evaluate: {files_to_evaluate}\")\n",
    "len(files_to_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe60e5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkEvalBaseBuilder:\n",
    "    def __init__(\n",
    "            self,\n",
    "            llm,\n",
    "            output_path: str,\n",
    "            builder_config: dict = {\n",
    "                'max_pages': 15,\n",
    "                'eval_batch_size': 5\n",
    "            },\n",
    "            prompts_config: dict = {\n",
    "                'path': '../prompts',\n",
    "                'random_queries': 'random_queries',\n",
    "                'paper_queries': 'paper_queries',\n",
    "                'chunk_eval_system': 'chunk_eval_system',\n",
    "                'chunk_eval_task': 'chunk_eval_task',\n",
    "                'doc_context_system': 'doc_context_system',\n",
    "                'doc_context_update': 'doc_context_update',\n",
    "                'doc_context_aggregation': 'doc_context_aggregation'\n",
    "            },\n",
    "            doc_search_config: dict = {\n",
    "                'main_queries_num': 4,\n",
    "                'paper_queries_num': 5,\n",
    "                'max_results': 5,\n",
    "            },\n",
    "            chunk_eval_config: dict = {\n",
    "                'chunk_size': 600,\n",
    "                'chunk_overlap': 0,\n",
    "                'max_queries': 5,\n",
    "                'context_agg_interval': 5\n",
    "            },\n",
    "            memory=None,\n",
    "            prompt_manager_spec: dict = {}\n",
    "        ):\n",
    "\n",
    "        self.llm = llm\n",
    "        self.output_path = output_path\n",
    "        self.builder_config = builder_config\n",
    "        self.prompts_config = prompts_config\n",
    "        self.doc_search_config = doc_search_config\n",
    "        self.chunk_eval_config = chunk_eval_config\n",
    "        self.memory = memory\n",
    "        self.prompt_manager_spec = prompt_manager_spec\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        self.docs_metadata_path = os.path.join(self.output_path, 'docs_metadata')\n",
    "        self.docs_path = os.path.join(self.output_path, 'docs')\n",
    "        self.eval_path = os.path.join(self.output_path, 'chunks_eval')\n",
    "\n",
    "        self.doc_loader = RandomQueriesPaperSearchGraph(\n",
    "            llm=self.llm,\n",
    "            prompts_config=self.prompts_config,\n",
    "            docs_path=self.docs_path,\n",
    "            docs_metadata_path=self.docs_metadata_path,\n",
    "            memory=self.memory,\n",
    "            prompt_manager_spec=self.prompt_manager_spec,\n",
    "            **self.doc_search_config\n",
    "        )\n",
    "\n",
    "        self.chunk_eval = ChunkEvalGraph(\n",
    "            llm=self.llm,\n",
    "            prompts_config=self.prompts_config,\n",
    "            docs_metadata_path=self.docs_metadata_path,\n",
    "            saving_path=self.eval_path,\n",
    "            memory=self.memory,\n",
    "            prompt_manager_spec=self.prompt_manager_spec,\n",
    "            **self.chunk_eval_config\n",
    "        )\n",
    "    \n",
    "    def _evaluated_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a list of evaluated documents.\n",
    "        \"\"\"\n",
    "        return get_filenames_list(self.eval_path)\n",
    "\n",
    "    def _docs_without_evaluation(self):\n",
    "        \"\"\"\n",
    "        Returns a list of documents that have not been evaluated yet.\n",
    "        \"\"\"\n",
    "        return get_filenames_without(self.docs_metadata_path, self._evaluated_docs())\n",
    "    \n",
    "    def _all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a list of all documents.\n",
    "        \"\"\"\n",
    "        return get_filenames_list(self.docs_metadata_path)\n",
    "    \n",
    "    @property\n",
    "    def docs_to_evaluate(self):\n",
    "        \"\"\"\n",
    "        Returns a number of documents that need to be evaluated.\n",
    "        \"\"\"\n",
    "        return len(self._docs_without_evaluation())\n",
    "    \n",
    "    @property\n",
    "    def docs_evaluated(self):\n",
    "        \"\"\"\n",
    "        Returns a number of documents that have been evaluated.\n",
    "        \"\"\"\n",
    "        return len(self._evaluated_docs())\n",
    "    \n",
    "    @property\n",
    "    def all_docs(self):\n",
    "        \"\"\"\n",
    "        Returns a number of all documents.\n",
    "        \"\"\"\n",
    "        return len(self._all_docs())\n",
    "    \n",
    "    @property\n",
    "    def new_docs_per_turn(self):\n",
    "        return self.doc_search_config['main_queries_num'] * self.doc_search_config['max_results']\n",
    "    \n",
    "    @property\n",
    "    def evaluations_per_turn(self):\n",
    "        return self.builder_config['eval_batch_size']\n",
    "    \n",
    "    @staticmethod\n",
    "    def _is_doc_oversized(path, pages_limit):\n",
    "        doc_metadata = load_json(path)\n",
    "        return doc_metadata['pages_count'] > pages_limit\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_json_extension(paths):\n",
    "        \"\"\"\n",
    "        Adds '.json' extension to a list of paths.\n",
    "        \"\"\"\n",
    "        return [f + '.json' for f in paths]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_pdf_extension(paths):\n",
    "        \"\"\"\n",
    "        Adds '.pdf' extension to a list of paths.\n",
    "        \"\"\"\n",
    "        return [f + '.pdf' for f in paths]\n",
    "    \n",
    "    def _remove_oversized_docs(self):\n",
    "        \"\"\"\n",
    "        Removes oversized documents based on the max_pages limit specified in the builder_config.\n",
    "        \"\"\"\n",
    "        if self.builder_config.get('max_pages'):\n",
    "            oversized_docs = [\n",
    "                f for f in self._docs_without_evaluation()\n",
    "                if self._is_doc_oversized(\n",
    "                    os.path.join(self.docs_metadata_path, f + '.json'),\n",
    "                    self.builder_config['max_pages']\n",
    "                )\n",
    "            ]\n",
    "            print(f\"Removing oversized docs: {oversized_docs}\")\n",
    "            docs_before = self.all_docs\n",
    "            remove_files(self.docs_metadata_path, self._add_json_extension(oversized_docs))\n",
    "            remove_files(self.docs_path, self._add_pdf_extension(oversized_docs))\n",
    "            docs_after = self.all_docs\n",
    "            docs_removed = docs_before - docs_after\n",
    "            print(f\"Removed {docs_removed} oversized docs. Remaining docs: {docs_after}\")\n",
    "\n",
    "    def extend_docs(self, target_docs: int):\n",
    "\n",
    "        current_docs_num = self.all_docs\n",
    "        docs_per_turn = self.new_docs_per_turn\n",
    "\n",
    "        turns_needed = math.ceil((target_docs - current_docs_num) / docs_per_turn)\n",
    "\n",
    "        print(f\"Current docs: {current_docs_num}, Target docs: {target_docs}, Turns needed: {turns_needed}\")\n",
    "\n",
    "        pbar = notebook_tqdm(total=turns_needed, desc=\"Collecting docs\", postfix={'All docs': current_docs_num, 'New docs': 0})\n",
    "\n",
    "        for _ in range(turns_needed):\n",
    "\n",
    "            try:\n",
    "                thread_id = uuid.uuid4().hex\n",
    "                self.doc_loader.run(thread_id=thread_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during doc loading: {e}\")\n",
    "            \n",
    "            all_docs = self.all_docs\n",
    "            new_docs = all_docs - current_docs_num\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'All docs': all_docs, 'New docs': new_docs})\n",
    "\n",
    "    \"\"\"async def evaluate_docs(self, target_docs: int):\n",
    "\n",
    "        self._remove_oversized_docs()\n",
    "\n",
    "        batch_size = self.evaluations_per_turn\n",
    "        turns_needed = math.ceil((target_docs - self.docs_evaluated) / batch_size)\n",
    "        docs_paths = self._add_json_extension(self._docs_without_evaluation())\n",
    "\n",
    "        async def run_query_async(metadata_file: str):\n",
    "            thread_id = uuid.uuid4().hex\n",
    "            state = await self.chunk_eval.run_with_progress_async(metadata_file=metadata_file, thread_id=thread_id)\n",
    "            return metadata_file, state\n",
    "        \n",
    "        process_pbar = notebook_tqdm(total=turns_needed, desc=\"Evaluating docs\", postfix={'Target Docs': target_docs, 'Evaluated': self.docs_evaluated})\n",
    "        \n",
    "        for _ in range(turns_needed):\n",
    "\n",
    "            batch_files = docs_paths[:batch_size]\n",
    "            docs_paths = docs_paths[batch_size:]\n",
    "\n",
    "            tasks = [run_query_async(metadata_file) for metadata_file in batch_files]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "\n",
    "            process_pbar.set_postfix({'Target Docs': target_docs, 'Evaluated': self.docs_evaluated})\"\"\"\n",
    "    \n",
    "    async def evaluate_docs(self, target_docs: int):\n",
    "        self._remove_oversized_docs()\n",
    "\n",
    "        batch_size = self.evaluations_per_turn\n",
    "        docs_needed = max(target_docs - self.docs_evaluated, 0)\n",
    "        docs_paths = self._add_json_extension(self._docs_without_evaluation())[:docs_needed]\n",
    "        total_docs = len(docs_paths)\n",
    "\n",
    "        # Progress bar\n",
    "        process_pbar = notebook_tqdm(total=total_docs, desc=\"Evaluating docs\", postfix={'Target Docs': target_docs, 'Evaluated': self.docs_evaluated})\n",
    "\n",
    "        async def worker(queue):\n",
    "            while True:\n",
    "                metadata_file = await queue.get()\n",
    "                if metadata_file is None:  # Sentinel to stop the worker\n",
    "                    break\n",
    "                try:\n",
    "                    thread_id = uuid.uuid4().hex\n",
    "                    state = await self.chunk_eval.run_with_progress_async(metadata_file=metadata_file, thread_id=thread_id)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {metadata_file}: {e}\")\n",
    "                finally:\n",
    "                    process_pbar.update(1)\n",
    "                    process_pbar.set_postfix({'Target Docs': target_docs, 'Evaluated': self.docs_evaluated})\n",
    "                    queue.task_done()\n",
    "\n",
    "        # Create a queue and populate it with files to process\n",
    "        queue = asyncio.Queue()\n",
    "        for metadata_file in docs_paths:\n",
    "            await queue.put(metadata_file)\n",
    "\n",
    "        # Start worker tasks\n",
    "        num_workers = batch_size  # Number of concurrent workers\n",
    "        workers = [asyncio.create_task(worker(queue)) for _ in range(num_workers)]\n",
    "\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        await queue.join()\n",
    "\n",
    "        # Stop workers\n",
    "        for _ in range(num_workers):\n",
    "            await queue.put(None)\n",
    "        await asyncio.gather(*workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b38f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_builder = ChunkEvalBaseBuilder(\n",
    "    llm=get_llm(llm_model='google'),\n",
    "    output_path='../data',\n",
    "    builder_config={\n",
    "        'max_pages': 15,\n",
    "        'eval_batch_size': 8\n",
    "    },\n",
    "    prompts_config={\n",
    "        'path': '../prompts',\n",
    "        'random_queries': 'random_queries',\n",
    "        'paper_queries': 'paper_queries',\n",
    "        'chunk_eval_system': 'chunk_eval_system',\n",
    "        'chunk_eval_task': 'chunk_eval_task',\n",
    "        'doc_context_system': 'doc_context_system',\n",
    "        'doc_context_update': 'doc_context_update',\n",
    "        'doc_context_aggregation': 'doc_context_aggregation'\n",
    "    },\n",
    "    doc_search_config={\n",
    "        'main_queries_num': 4,\n",
    "        'paper_queries_num': 5,\n",
    "        'max_results': 5,\n",
    "    },\n",
    "    chunk_eval_config={\n",
    "        'chunk_size': 600,\n",
    "        'chunk_overlap': 0,\n",
    "        'max_queries': 5,\n",
    "        'context_agg_interval': 5\n",
    "    },\n",
    "    memory=get_checkpointer(checkpointer_mode='local')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50abd049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current docs: 0, Target docs: 25, Turns needed: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5184c9d103b46c08d0274aa392cf44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting docs:   0%|          | 0/3 [00:00<?, ?it/s, All docs=0, New docs=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for papers with query: exoplanet atmospheric characterization JWST NIRSpec\n",
      "Searching for papers with query: high-redshift galaxy formation simulations feedback\n",
      "Searching for papers with query: CRISPR-Cas gene editing therapy ethical implications\n",
      "Searching for papers with query: topological insulator quantum computing Majorana fermions\n",
      "Searching for papers with query: federated learning privacy attacks defense mechanisms\n",
      "Searching for papers with query: explainable AI methods for medical image diagnosis\n",
      "Searching for papers with query: CRISPR gene editing ethical considerations review\n",
      "Searching for papers with query: blockchain technology supply chain management applications\n",
      "Searching for papers with query: graphene based photodetectors for infrared imaging\n",
      "Searching for papers with query: explainable AI methods for fraud detection in finance\n",
      "Searching for papers with query: federated learning privacy preserving techniques healthcare\n",
      "Searching for papers with query: blockchain technology for supply chain management traceability\n"
     ]
    }
   ],
   "source": [
    "base_builder.extend_docs(target_docs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92add39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing oversized docs: []\n",
      "Removed 0 oversized docs. Remaining docs: 35\n"
     ]
    }
   ],
   "source": [
    "base_builder._remove_oversized_docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf61c82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_builder.docs_evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67a5f647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing oversized docs: []\n",
      "Removed 0 oversized docs. Remaining docs: 35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90fea99dfd3547a6b3593fc5c273b7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating docs:   0%|          | 0/14 [00:00<?, ?it/s, Evaluated=11, Target Docs=25]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 11 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 23 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 47 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 55 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 77 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 126 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c5881a5af04280869d9cd3999a1f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/39 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 182 0 (offset 0)\n",
      "Ignoring wrong pointing object 184 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87228968933c4b76af91071551973d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/64 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 190 0 (offset 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419786220ee64a05a9dc93cee080a248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70137f5a690641418e7f2dc473ca0fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/84 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8569de32c0464453ac717255e4755c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/117 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe425670e674b2b9c211befd51dbd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/111 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cad4385e29495588cae17fc88d7843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/78 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1206a6144b37477a90b7b967086f72be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/43 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results on document ../data/docs/2403.01927v1.pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99167dc17824010a36d0ea9f68ab45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/77 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results on document ../data/docs/1507.02655v1.pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ee656d88a4ac79eab3a415a2d0607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/30 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results on document ../data/docs/2209.04917v1.pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae7214e88a74a6da7e89439c91e5348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/100 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results on document ../data/docs/2501.06887v1.pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed44617a4654c43837b73d4900ddb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/53 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results on document ../data/docs/0903.2196v1.pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ace853d88dc49438af9c7b6595d26bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/62 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results on document ../data/docs/2206.00769v1.pdf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c2b8fd65e943e2b4994c3f18961de1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/32 [00:00<?, ?it/s, Negative=0, Positive=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results on document ../data/docs/1811.03230v1.pdf...\n",
      "Saving evaluation results on document ../data/docs/1810.09203v1.pdf...\n",
      "Saving evaluation results on document ../data/docs/2409.05938v1.pdf...\n",
      "Saving evaluation results on document ../data/docs/1310.3528v1.pdf...\n",
      "Saving evaluation results on document ../data/docs/2010.10572v1.pdf...\n",
      "Saving evaluation results on document ../data/docs/2312.00586v1.pdf...\n",
      "Saving evaluation results on document ../data/docs/2203.04173v1.pdf...\n",
      "Saving evaluation results on document ../data/docs/1906.01831v1.pdf...\n"
     ]
    }
   ],
   "source": [
    "await base_builder.evaluate_docs(target_docs=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
