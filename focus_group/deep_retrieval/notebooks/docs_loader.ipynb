{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7ac0a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage, AIMessage, AnyMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import List, Dict, Any, TypedDict, Annotated\n",
    "from pydantic import BaseModel\n",
    "from langgraph.constants import Send\n",
    "import json\n",
    "from operator import add\n",
    "from fcgb.cfg.precompiled import get_llm, get_checkpointer\n",
    "from fcgb.prompt_manager import PromptManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4573eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import urllib.request as libreq\n",
    "import feedparser\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "def sanitize_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Sanitizes a query string for API calls by removing special characters\n",
    "    and replacing white spaces with '+'.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query string.\n",
    "\n",
    "    Returns:\n",
    "        str: The sanitized query string.\n",
    "    \"\"\"\n",
    "    # Remove special characters except alphanumeric and spaces\n",
    "    sanitized = re.sub(r'[^\\w\\s]', '', query)\n",
    "    # Replace white spaces with '+'\n",
    "    sanitized = sanitized.replace(' ', '+')\n",
    "    return sanitized\n",
    "\n",
    "def create_path_if_not_exists(path: str):\n",
    "    \"\"\"\n",
    "    Creates a directory path if it doesn't exist.\n",
    "\n",
    "    Args:\n",
    "        path (str): The directory path to create.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def arxiv_pdf_link_extractor(links: List[Dict[str, str]]) -> str | None:\n",
    "    \"\"\"\n",
    "    Extracts the PDF link from a list of links.\n",
    "    Args:\n",
    "        links (List[Dict[str, str]]): A list of dictionaries containing link information.\n",
    "    Returns:\n",
    "        str | None: The PDF link if found, otherwise None.\n",
    "    \"\"\"\n",
    "\n",
    "    link = [link['href'] for link in links if link.get('title', '') == 'pdf']\n",
    "    if len(link) > 0:\n",
    "        return link[0]\n",
    "    return None\n",
    "\n",
    "def arxiv_search(search_query: str, start: int = 0, max_results: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Searches the arXiv API for papers matching the search query.\n",
    "\n",
    "    Args:\n",
    "        search_query (str): The query to search for.\n",
    "        start (int): The starting index for results.\n",
    "        max_results (int): The maximum number of results to return.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries containing paper titles, summaries, publish dates and URLs.\n",
    "    \"\"\"\n",
    "    api_call = 'http://export.arxiv.org/api/query?search_query=all:%s&start=%i&max_results=%i' % (\n",
    "        sanitize_query(search_query), start, max_results)\n",
    "    try:\n",
    "        response = libreq.urlopen(api_call)\n",
    "        if response.status != 200:\n",
    "            raise Exception(f\"Error fetching data from arXiv API: {response.status}\")\n",
    "        results = feedparser.parse(response.read())\n",
    "        docs = [{\n",
    "            'title': entry.title, \n",
    "            'summary': entry.summary,\n",
    "            'published': entry.published,\n",
    "            'url': arxiv_pdf_link_extractor(entry.links)\n",
    "            } for entry in results.entries]\n",
    "        return [doc for doc in docs if doc['url'] is not None]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return []\n",
    "    \n",
    "\n",
    "def download_pdf(url: str, save_dir: str):\n",
    "    \"\"\"\n",
    "    Downloads a PDF file from a URL to a specified path.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the PDF file.\n",
    "        save_path (str): The path where the PDF file will be saved.\n",
    "    Returns:\n",
    "        str: The path where the PDF file was saved.\n",
    "    \"\"\"\n",
    "    # Extract the filename from the URL\n",
    "    filename = os.path.basename(urlparse(url).path) + '.pdf'\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Failed to download file: {response.status_code}\")\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                file.write(chunk)\n",
    "        return save_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading PDF: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "28b922d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class arXivDocSpec(TypedDict):\n",
    "    title: str\n",
    "    summary: str\n",
    "    published: str\n",
    "    url: str\n",
    "\n",
    "class PapersRetrieveGraphState(BaseModel):\n",
    "    search_query: str\n",
    "    docs_specs: List[arXivDocSpec] = []\n",
    "    metadata_files: Annotated[List[str], add] = []\n",
    "\n",
    "class PapersRetrieveGraph:\n",
    "    def __init__(\n",
    "            self,\n",
    "            docs_path,\n",
    "            docs_metadata_path,\n",
    "            memory=None,\n",
    "            max_results: int = 5,\n",
    "    ):\n",
    "        self.docs_path = docs_path\n",
    "        self.docs_metadata_path = docs_metadata_path\n",
    "        self.memory = memory\n",
    "        self.max_results = max_results\n",
    "\n",
    "        create_path_if_not_exists(self.docs_path)\n",
    "        create_path_if_not_exists(self.docs_metadata_path)\n",
    "\n",
    "        self.build_graph()\n",
    "\n",
    "    def search_node(self, state: PapersRetrieveGraphState):\n",
    "        \"\"\"\n",
    "        Searches for papers on arXiv based on the search query in the state.\n",
    "        \"\"\"\n",
    "        print(f\"Searching for papers with query: {state.search_query}\")\n",
    "        docs_specs = arxiv_search(state.search_query, max_results=self.max_results, start=0)\n",
    "        return {'docs_specs': docs_specs}\n",
    "    \n",
    "    def download_router(self, state: PapersRetrieveGraphState):\n",
    "        \"\"\"\n",
    "        Distributes docs specifications into download nodes.\n",
    "        \"\"\"\n",
    "        return [Send(\"doc_downloader\", doc_spec) for doc_spec in state.docs_specs]\n",
    "    \n",
    "    def doc_download_node(self, doc_spec: arXivDocSpec):\n",
    "        \"\"\"\n",
    "        Downloads a PDF file based on the document specification provided and saves its metadata.\n",
    "        \"\"\"\n",
    "        print(f\"Downloading PDF for: {doc_spec['title']}\")\n",
    "        result = download_pdf(doc_spec['url'], self.docs_path)\n",
    "        if result is not None:\n",
    "            doc_spec['path'] = result\n",
    "            # Get pdf file pages count with PyMuPDF\n",
    "            try:\n",
    "                pdf_document = fitz.open(result)\n",
    "                doc_spec['pages_count'] = pdf_document.page_count\n",
    "                pdf_document.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading PDF file {result}: {e}\")\n",
    "                doc_spec['pages_count'] = None\n",
    "            # Save doc_spec to metadata json file\n",
    "            metadata_file = os.path.join(self.docs_metadata_path, f\"{'.'.join(os.path.basename(result).split('.')[:-1])}.json\")\n",
    "            with open(metadata_file, 'w') as f:\n",
    "                json.dump(doc_spec, f, indent=4)\n",
    "            return {'metadata_files': [metadata_file]}\n",
    "        return {'metadata_files': []}\n",
    "    \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Builds the state graph for retrieving papers from arXiv.\n",
    "        \"\"\"\n",
    "        workflow = StateGraph(PapersRetrieveGraphState)\n",
    "        workflow.add_node('search_docs', self.search_node)\n",
    "        workflow.add_node('doc_downloader', self.doc_download_node)\n",
    "\n",
    "        workflow.add_edge(START, 'search_docs')\n",
    "        workflow.add_conditional_edges('search_docs', self.download_router, ['doc_downloader'])\n",
    "        workflow.add_edge('doc_downloader', END)\n",
    "\n",
    "        self.graph = workflow.compile(checkpointer=self.memory)\n",
    "\n",
    "    def run(self, search_query: str, thread_id: str = None):\n",
    "        \"\"\"\n",
    "        Runs the graph with the provided search query.\n",
    "\n",
    "        Args:\n",
    "            search_query (str): The query to search for papers on arXiv.\n",
    "        \"\"\"\n",
    "        config = {'configurable': {'thread_id': thread_id}} if thread_id else None\n",
    "        return self.graph.invoke({'search_query': search_query}, config=config)\n",
    "\n",
    "class QueriesModel(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "class RandomQueriesPaperSearchGraphState(BaseModel):\n",
    "    queries: QueriesModel = []\n",
    "    metadata_files: Annotated[List[str], add] = []\n",
    "\n",
    "\n",
    "class RandomQueriesPaperSearchGraph:\n",
    "    \"\"\"\n",
    "    A graph for generating random queries and searching for papers on arXiv.\n",
    "    This graph generates random queries, retrieves papers based on those queries,\n",
    "    and generates additional queries based on the retrieved papers.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            llm,\n",
    "            prompts_config: Dict,\n",
    "            docs_path: str,\n",
    "            docs_metadata_path: str,\n",
    "            memory=None,\n",
    "            main_queries_num: int = 5,\n",
    "            paper_queries_num: int = 10,\n",
    "            max_results: int = 5,\n",
    "            prompt_manager_spec: Dict = {}\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the RandomQueriesPaperSearchGraph.\n",
    "        Args:\n",
    "            llm: The language model to use for generating queries.\n",
    "            prompts_config (Dict): Configuration for prompts.\n",
    "                Configuration should include:\n",
    "                'path': main path where prompts are stored,\n",
    "                'random_queries': title of the prompt for generating random queries,\n",
    "                'paper_queries': title of the prompt for generating queries based on papers.\n",
    "            docs_path (str): Path to save downloaded documents.\n",
    "            docs_metadata_path (str): Path to save metadata of downloaded documents.\n",
    "            memory: Optional memory for the graph.\n",
    "            main_queries_num (int): Number of main queries to generate.\n",
    "            paper_queries_num (int): Number of queries to generate for each paper.\n",
    "            max_results (int): Maximum number of results to retrieve from arXiv.\n",
    "            prompt_manager_spec (Dict): Specification for the prompt manager, especially prompts versions.\n",
    "                ex.\n",
    "                {'version_config': {'random_queries': 'v1.0', 'paper_queries': 'v1.1'}}\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompts_config = prompts_config\n",
    "        self.docs_path = docs_path\n",
    "        self.docs_metadata_path = docs_metadata_path\n",
    "        self.memory = memory\n",
    "        self.main_queries_num = main_queries_num\n",
    "        self.paper_queries_num = paper_queries_num\n",
    "        self.prompt_manager_spec = prompt_manager_spec\n",
    "        self.max_results = max_results\n",
    "\n",
    "        create_path_if_not_exists(self.docs_path)\n",
    "        create_path_if_not_exists(self.docs_metadata_path)\n",
    "\n",
    "        self._set_promps()\n",
    "        self._set_downloader()\n",
    "\n",
    "        self.build_graph()\n",
    "\n",
    "    def _set_downloader(self):\n",
    "        \"\"\"\n",
    "        Initializes the downloader graph for retrieving papers from arXiv.\n",
    "        \"\"\"\n",
    "        self.downloader = PapersRetrieveGraph(\n",
    "            docs_path=self.docs_path,\n",
    "            docs_metadata_path=self.docs_metadata_path,\n",
    "            max_results=self.max_results\n",
    "        )\n",
    "\n",
    "    def _set_promps(self):\n",
    "        \"\"\"\n",
    "        Initializes the prompt manager and retrieves the prompts for generating queries.\n",
    "        \"\"\"\n",
    "        prompt_manager = PromptManager(\n",
    "            version_config=self.prompt_manager_spec,\n",
    "            path=self.prompts_config['path']\n",
    "        )\n",
    "\n",
    "        self.prompts = prompt_manager.get_prompts([self.prompts_config[name] for name in ['random_queries', 'paper_queries']])\n",
    "\n",
    "    def main_queries_node(self, state: RandomQueriesPaperSearchGraphState):\n",
    "        \"\"\"\n",
    "        Generates a set of random queries for searching papers.\n",
    "        \"\"\"    \n",
    "        print(\"Generating random queries for paper search...\")\n",
    "        query_prompt = self.prompts['random_queries'].format(\n",
    "            main_queries_num=self.main_queries_num\n",
    "        )\n",
    "        queries = self.llm.with_structured_output(QueriesModel).invoke(query_prompt)\n",
    "        return {'queries': queries}\n",
    "    \n",
    "    def query_routing(self, state: RandomQueriesPaperSearchGraphState):\n",
    "        \"\"\"\n",
    "        Distributes queries into paper search nodes.\n",
    "        \"\"\"\n",
    "        return [Send(\"paper_search\", {'search_query': query}) for query in state.queries.queries]\n",
    "    \n",
    "    def papers_routing(self, state: RandomQueriesPaperSearchGraphState):\n",
    "        \"\"\"\n",
    "        Distributes metadata files paths into paper queries nodes.\n",
    "        \"\"\"\n",
    "        return [Send(\"paper_queries\", file) for file in state.metadata_files]\n",
    "    \n",
    "    def paper_queries(self, metadata_path: str):\n",
    "        \"\"\"\n",
    "        Generates paper queries based on the metadata file.\n",
    "        \"\"\"\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        print(f\"Generating queries for paper: {metadata['title']}\")\n",
    "        \n",
    "        query_prompt = self.prompts['paper_queries'].format(\n",
    "            title=metadata['title'],\n",
    "            summary=metadata['summary'],\n",
    "            paper_queries_num=self.paper_queries_num\n",
    "        )\n",
    "        \n",
    "        queries = self.llm.with_structured_output(QueriesModel).invoke(query_prompt)\n",
    "\n",
    "        metadata['queries'] = queries.queries\n",
    "        # Save updated metadata with queries\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        return {}\n",
    "    \n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Builds the state graph for retrieving papers based on random queries.\n",
    "        \"\"\"\n",
    "        workflow = StateGraph(RandomQueriesPaperSearchGraphState)\n",
    "        workflow.add_node('main_queries', self.main_queries_node)\n",
    "        workflow.add_node('paper_search', self.downloader.graph)\n",
    "        workflow.add_node('paper_queries', self.paper_queries)\n",
    "\n",
    "        workflow.add_edge(START, 'main_queries')\n",
    "        workflow.add_conditional_edges('main_queries', self.query_routing, ['paper_search'])\n",
    "        workflow.add_conditional_edges('paper_search', self.papers_routing, ['paper_queries'])\n",
    "        workflow.add_edge('paper_queries', END)\n",
    "\n",
    "        self.graph = workflow.compile(checkpointer=self.memory)\n",
    "\n",
    "    def run(self, thread_id: str = None):\n",
    "        \"\"\"\n",
    "        Runs the graph to generate random queries and search for papers.\n",
    "\n",
    "        Args:\n",
    "            thread_id (str): Optional thread ID for memory management.\n",
    "        \"\"\"\n",
    "        config = {'configurable': {'thread_id': thread_id}} if thread_id else None\n",
    "        return self.graph.invoke({}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1e7b255",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader = PapersRetrieveGraph(\n",
    "    docs_path='../docs',\n",
    "    docs_metadata_path='../docs_metadata',\n",
    "    max_results=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432f61e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for papers with query: Chain of Thought\n",
      "Downloading PDF for: Contrastive Chain-of-Thought Prompting\n",
      "Downloading PDF for: Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\n",
      "  Language Models\n",
      "Downloading PDF for: CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A\n",
      "  Theory Perspective\n",
      "Error downloading PDF: Failed to download file: 404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'search_query': 'Chain of Thought',\n",
       " 'docs_specs': [{'title': 'Contrastive Chain-of-Thought Prompting',\n",
       "   'summary': 'Despite the success of chain of thought in enhancing language model\\nreasoning, the underlying process remains less well understood. Although\\nlogically sound reasoning appears inherently crucial for chain of thought,\\nprior studies surprisingly reveal minimal impact when using invalid\\ndemonstrations instead. Furthermore, the conventional chain of thought does not\\ninform language models on what mistakes to avoid, which potentially leads to\\nmore errors. Hence, inspired by how humans can learn from both positive and\\nnegative examples, we propose contrastive chain of thought to enhance language\\nmodel reasoning. Compared to the conventional chain of thought, our approach\\nprovides both valid and invalid reasoning demonstrations, to guide the model to\\nreason step-by-step while reducing reasoning mistakes. To improve\\ngeneralization, we introduce an automatic method to construct contrastive\\ndemonstrations. Our experiments on reasoning benchmarks demonstrate that\\ncontrastive chain of thought can serve as a general enhancement of\\nchain-of-thought prompting.',\n",
       "   'published': '2023-11-15T18:54:01Z',\n",
       "   'url': 'http://arxiv.org/pdf/2311.09277v1'},\n",
       "  {'title': 'Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in\\n  Language Models',\n",
       "   'summary': \"With the widespread use of language models (LMs) in NLP tasks, researchers\\nhave discovered the potential of Chain-of-thought (CoT) to assist LMs in\\naccomplishing complex reasoning tasks by generating intermediate steps.\\nHowever, human thought processes are often non-linear, rather than simply\\nsequential chains of thoughts. Therefore, we propose Graph-of-Thought (GoT)\\nreasoning, which models human thought processes not only as a chain but also as\\na graph. By representing thought units as nodes and connections between them as\\nedges, our approach captures the non-sequential nature of human thinking and\\nallows for a more realistic modeling of thought processes. GoT adopts a\\ntwo-stage framework with an additional GoT encoder for thought graph\\nrepresentation and fuses the graph representation with the original input\\nrepresentation through a gated fusion mechanism. We evaluate GoT's performance\\non a text-only reasoning task (AQUA-RAT) and a multimodal reasoning task\\n(ScienceQA). Our model achieves significant improvement over the strong CoT\\nbaseline on the AQUA-RAT test set and boosts accuracy from 85.19% to 87.59%\\nusing the T5-base model over the state-of-the-art Multimodal-CoT on the\\nScienceQA test set.\",\n",
       "   'published': '2023-05-26T02:15:09Z',\n",
       "   'url': 'http://arxiv.org/pdf/2305.16582v2'},\n",
       "  {'title': 'CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A\\n  Theory Perspective',\n",
       "   'summary': 'Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\\nLarge Language Models on tasks requiring multi-step inference. This success has\\nled to widespread claims of emergent reasoning capabilities in these models. In\\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\\nChain-of-Thought functions as a powerful structural constraint that guides\\nLarge Language Models to imitate the form of reasoning. By forcing the\\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\\ncapacity for sequence prediction and pattern matching, effectively constraining\\nits output to sequences that resemble coherent thought processes.\\nChain-of-Thought (CoT) prompting has demonstrably enhanced the performance of\\nLarge Language Models on tasks requiring multi-step inference. This success has\\nled to widespread claims of emergent reasoning capabilities in these models. In\\nthis paper, we present a theoretical counter-perspective: Chain-of-Thought\\n(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that\\nChain-of-Thought functions as a powerful structural constraint that guides\\nLarge Language Models to imitate the form of reasoning. By forcing the\\ngeneration of intermediate steps, Chain-of-Thought leverages the model immense\\ncapacity for sequence prediction and pattern matching, effectively constraining\\nits output to sequences that resemble coherent thought processes.',\n",
       "   'published': '2025-06-03T13:45:01Z',\n",
       "   'url': 'http://arxiv.org/pdf/2506.02878v2'}],\n",
       " 'metadata_files': ['../docs_metadata/2311.json',\n",
       "  '../docs_metadata/2305.json']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloader.run(search_query='Chain of Thought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "223a807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_runner = RandomQueriesPaperSearchGraph(\n",
    "    llm=get_llm(llm_model='google'),\n",
    "    prompts_config={\n",
    "        'path': '../prompts',\n",
    "        'random_queries': 'random_queries',\n",
    "        'paper_queries': 'paper_queries'\n",
    "    },\n",
    "    docs_path='../docs',\n",
    "    docs_metadata_path='../docs_metadata',\n",
    "    main_queries_num=3,\n",
    "    paper_queries_num=10,\n",
    "    max_results=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "238d400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random queries for paper search...\n",
      "Searching for papers with query: explainable AI methods for fraud detection in banking\n",
      "Searching for papers with query: federated learning privacy preserving techniques healthcare\n",
      "Searching for papers with query: transformer networks time series forecasting stock market\n",
      "Downloading PDF for: MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\n",
      "Downloading PDF for: Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field\n",
      "  and CNNs for Stock Return Predictions\n",
      "Downloading PDF for: An Evaluation of Deep Learning Models for Stock Market Trend Prediction\n",
      "Downloading PDF for: Transformer Based Time-Series Forecasting for Stock\n",
      "Downloading PDF for: Stock Market Telepathy: Graph Neural Networks Predicting the Secret\n",
      "  Conversations between MINT and G7 Countries\n",
      "Downloading PDF for: Vision Through the Veil: Differential Privacy in Federated Learning for\n",
      "  Medical Image Classification\n",
      "Downloading PDF for: Robust Aggregation for Adaptive Privacy Preserving Federated Learning in\n",
      "  Healthcare\n",
      "Downloading PDF for: Privacy Preserving Machine Learning for Electronic Health Records using\n",
      "  Federated Learning and Differential Privacy\n",
      "Downloading PDF for: Complex-valued Federated Learning with Differential Privacy and MRI\n",
      "  Applications\n",
      "Downloading PDF for: Blockchain Integrated Federated Learning in Edge-Fog-Cloud Systems for\n",
      "  IoT based Healthcare Applications A Survey\n",
      "Downloading PDF for: SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative\n",
      "  Mask Learning\n",
      "Downloading PDF for: Computer-Assisted Fraud Detection, From Active Learning to Reward\n",
      "  Maximization\n",
      "Downloading PDF for: A novel approach to increase scalability while training machine learning\n",
      "  algorithms using Bfloat 16 in credit card fraud detection\n",
      "Downloading PDF for: Credit Card Fraud Detection: A Deep Learning Approach\n",
      "Downloading PDF for: Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud\n",
      "  Detection\n",
      "Generating queries for paper: A novel approach to increase scalability while training machine learning\n",
      "  algorithms using Bfloat 16 in credit card fraud detection\n",
      "Generating queries for paper: SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative\n",
      "  Mask Learning\n",
      "Generating queries for paper: Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud\n",
      "  Detection\n",
      "Generating queries for paper: Vision Through the Veil: Differential Privacy in Federated Learning for\n",
      "  Medical Image Classification\n",
      "Generating queries for paper: Computer-Assisted Fraud Detection, From Active Learning to Reward\n",
      "  Maximization\n",
      "Generating queries for paper: Complex-valued Federated Learning with Differential Privacy and MRI\n",
      "  Applications\n",
      "Generating queries for paper: Privacy Preserving Machine Learning for Electronic Health Records using\n",
      "  Federated Learning and Differential Privacy\n",
      "Generating queries for paper: Credit Card Fraud Detection: A Deep Learning Approach\n",
      "Generating queries for paper: Blockchain Integrated Federated Learning in Edge-Fog-Cloud Systems for\n",
      "  IoT based Healthcare Applications A Survey\n",
      "Generating queries for paper: MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\n",
      "Generating queries for paper: Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field\n",
      "  and CNNs for Stock Return Predictions\n",
      "Generating queries for paper: Robust Aggregation for Adaptive Privacy Preserving Federated Learning in\n",
      "  Healthcare\n",
      "Generating queries for paper: An Evaluation of Deep Learning Models for Stock Market Trend Prediction\n",
      "Generating queries for paper: Transformer Based Time-Series Forecasting for Stock\n",
      "Generating queries for paper: Stock Market Telepathy: Graph Neural Networks Predicting the Secret\n",
      "  Conversations between MINT and G7 Countries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'queries': QueriesModel(queries=['explainable AI methods for fraud detection in banking', 'federated learning privacy preserving techniques healthcare', 'transformer networks time series forecasting stock market']),\n",
       " 'metadata_files': ['../docs_metadata/2406.11389v1.json',\n",
       "  '../docs_metadata/1811.08212v1.json',\n",
       "  '../docs_metadata/2206.12415v1.json',\n",
       "  '../docs_metadata/2409.13406v1.json',\n",
       "  '../docs_metadata/2108.02501v3.json',\n",
       "  '../docs_metadata/2306.17794v1.json',\n",
       "  '../docs_metadata/2009.08294v1.json',\n",
       "  '../docs_metadata/2406.15962v1.json',\n",
       "  '../docs_metadata/2110.03478v2.json',\n",
       "  '../docs_metadata/2406.05517v1.json',\n",
       "  '../docs_metadata/2312.15235v1.json',\n",
       "  '../docs_metadata/2310.07427v3.json',\n",
       "  '../docs_metadata/2408.12408v1.json',\n",
       "  '../docs_metadata/2502.09625v1.json',\n",
       "  '../docs_metadata/2506.01945v1.json']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab99d9",
   "metadata": {},
   "source": [
    "## Chunks labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f228e665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkEvalModel(TypedDict):\n",
    "    idx: int\n",
    "    text: str\n",
    "    label: bool\n",
    "\n",
    "class ChunkEvalLabel(BaseModel):\n",
    "    evaluation: bool\n",
    "\n",
    "class ChunkEvalState(BaseModel):\n",
    "    query: str\n",
    "    query_idx: int\n",
    "    doc_file: str\n",
    "    title: str\n",
    "    summary: str\n",
    "    context: str = ''\n",
    "    chunks: List[str] = []\n",
    "    chunks_num: int = 0\n",
    "    chunk_idx: int = 0\n",
    "    chunks_eval: Annotated[List[ChunkEvalModel], add] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9eccfd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from tqdm import tqdm\n",
    "from tqdm.asyncio import tqdm as async_tqdm\n",
    "from tqdm.notebook import tqdm as notebook_tqdm\n",
    "import asyncio\n",
    "\n",
    "class ChunkEvalGraph:\n",
    "    def __init__(\n",
    "            self,\n",
    "            llm,\n",
    "            prompts_config: Dict,\n",
    "            docs_metadata_path: str,\n",
    "            saving_path: str,\n",
    "            memory=None,\n",
    "            chunk_size: int = 600,\n",
    "            chunk_overlap: int = 0,\n",
    "            context_agg_interval: int = 5,\n",
    "            prompt_manager_spec: Dict = {}\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the ChunkEvalGraph for evaluating chunks of text based on a query.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model to use for evaluating chunks.\n",
    "            prompts_config (Dict): Configuration for prompts.\n",
    "                Configuration should include:\n",
    "                'path': main path where prompts are stored,\n",
    "                'chunk_eval_system': title of the system prompt for chunk evaluation.\n",
    "                'chunk_eval_task': title of the prompt for chunk evaluation task.\n",
    "                'doc_context_system': title of the prompt for document's current context system message.\n",
    "                'doc_context_update': title of the prompt for updating document's current context.\n",
    "                'doc_context_aggregation': title of the prompt for aggregating document's context.\n",
    "            docs_metadata_path (str): Path to save metadata of downloaded documents.\n",
    "            saving_path (str): Path to save evaluation results.\n",
    "            memory: Optional memory for the graph.\n",
    "            chunk_size (int): Size of each chunk to evaluate.\n",
    "            chunk_overlap (int): Overlap between chunks.\n",
    "            context_agg_interval (int): Interval for aggregating context.\n",
    "            prompt_manager_spec (Dict): Specification for the prompt manager, especially prompts versions.\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.prompts_config = prompts_config\n",
    "        self.docs_metadata_path = docs_metadata_path\n",
    "        self.saving_path = saving_path\n",
    "        self.memory = memory\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.context_agg_interval = context_agg_interval\n",
    "        self.prompt_manager_spec = prompt_manager_spec\n",
    "\n",
    "        create_path_if_not_exists(self.saving_path)\n",
    "\n",
    "        self._set_text_splitter()\n",
    "        self._set_promps()\n",
    "        self.build_graph()\n",
    "    \n",
    "    def _set_promps(self):\n",
    "        \"\"\"\n",
    "        Initializes the prompt manager and retrieves the prompts for chunk evaluation.\n",
    "        \"\"\"\n",
    "        prompt_manager = PromptManager(\n",
    "            version_config=self.prompt_manager_spec,\n",
    "            path=self.prompts_config['path']\n",
    "        )\n",
    "\n",
    "        self.prompts = prompt_manager.get_prompts([self.prompts_config[name] for name in ['chunk_eval_system', 'chunk_eval_task', 'doc_context_system', 'doc_context_update', 'doc_context_aggregation']])\n",
    "\n",
    "    def _set_text_splitter(self):\n",
    "        \"\"\"\n",
    "        Initializes the text splitter for chunking documents.\n",
    "        \"\"\"\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "\n",
    "    def chunking_node(self, state: ChunkEvalState):\n",
    "        \n",
    "        try:\n",
    "            loader = PyPDFLoader(state.doc_file, mode='single', extraction_mode='plain')\n",
    "            content = loader.load()[0].page_content\n",
    "            chunks = self.text_splitter.split_text(content)\n",
    "            \n",
    "            chunks_num = len(chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading or chunking document {state.doc_file}: {e}\")\n",
    "            chunks = []\n",
    "            chunks_num = 0\n",
    "\n",
    "        #print(f\"Chunking document: {state.doc_file} into {chunks_num} chunks.\")\n",
    "\n",
    "        return {\n",
    "            'chunks': chunks,\n",
    "            'chunks_num': chunks_num,\n",
    "            'chunk_idx': 0,\n",
    "            'context': '',\n",
    "        }\n",
    "    \n",
    "    def _get_template_inputs(self, state: ChunkEvalState):\n",
    "        \"\"\"\n",
    "        Prepares the template inputs for the system and task messages.\n",
    "        \"\"\"\n",
    "        prev_chunk = state.chunks[state.chunk_idx - 1] if state.chunk_idx > 0 else ''\n",
    "        current_chunk = state.chunks[state.chunk_idx] if state.chunk_idx < state.chunks_num else ''\n",
    "        next_chunk = state.chunks[state.chunk_idx + 1] if state.chunk_idx + 1 < state.chunks_num else ''\n",
    "\n",
    "        return {\n",
    "            'title': state.title,\n",
    "            'summary': state.summary,\n",
    "            'query': state.query,\n",
    "            'context': state.context,\n",
    "            'prev_chunk': prev_chunk,\n",
    "            'current_chunk': current_chunk,\n",
    "            'next_chunk': next_chunk,\n",
    "            'current_chunk_index': state.chunk_idx + 1,\n",
    "            'total_chunks_num': state.chunks_num\n",
    "        }\n",
    "    \n",
    "    def chunk_eval_node(self, state: ChunkEvalState):\n",
    "        \"\"\"\n",
    "        Evaluates the current chunk based on the query and updates the context.\n",
    "        \"\"\"\n",
    "\n",
    "        template_inputs = self._get_template_inputs(state)\n",
    "        system_msg = SystemMessage(content=self.prompts['chunk_eval_system'].format(**template_inputs))\n",
    "        task_msg = HumanMessage(content=self.prompts['chunk_eval_task'].format(**template_inputs))\n",
    "\n",
    "        eval_result = self.llm.with_structured_output(ChunkEvalLabel).invoke([system_msg, task_msg]).evaluation\n",
    "\n",
    "        chunk_eval = ChunkEvalModel(\n",
    "            idx=state.chunk_idx,\n",
    "            text=template_inputs['current_chunk'],\n",
    "            label=eval_result\n",
    "        )\n",
    "\n",
    "        return {'chunks_eval': [chunk_eval]}\n",
    "    \n",
    "    def context_update_node(self, state: ChunkEvalState):\n",
    "        \"\"\"\n",
    "        Updates the context based on the evaluation of the current chunk.\n",
    "        \"\"\"\n",
    "        \n",
    "        template_inputs = self._get_template_inputs(state)\n",
    "        system_msg = SystemMessage(content=self.prompts['doc_context_system'].format(**template_inputs))\n",
    "        update_msg = HumanMessage(content=self.prompts['doc_context_update'].format(**template_inputs))\n",
    "\n",
    "        context_update = self.llm.invoke([system_msg, update_msg]).content\n",
    "\n",
    "        return {\n",
    "            'context': context_update,\n",
    "            'chunk_idx': state.chunk_idx + 1\n",
    "        }\n",
    "    \n",
    "    def context_aggregation_node(self, state: ChunkEvalState):\n",
    "        \"\"\"\n",
    "        Aggregates current summaries\n",
    "        \"\"\"\n",
    "        template_inputs = self._get_template_inputs(state)\n",
    "        agg_msg = HumanMessage(content=self.prompts['doc_context_aggregation'].format(**template_inputs))\n",
    "\n",
    "        context_update = self.llm.invoke([agg_msg]).content\n",
    "\n",
    "        return {'context': context_update}\n",
    "    \n",
    "    def context_aggregation_routing(self, state: ChunkEvalState):\n",
    "        \"\"\"\n",
    "        Determines whether to aggregate context based on the current chunk index.\n",
    "        If the current chunk index is a multiple of the context aggregation interval and greater than 0,\n",
    "        it returns 'context_aggregation', otherwise it returns 'chunk_eval_node'.\n",
    "        \"\"\"\n",
    "        if state.chunk_idx % self.context_agg_interval == 0 and state.chunk_idx > 0:\n",
    "            return 'context_aggregation'\n",
    "        return 'chunk_eval'\n",
    "        \n",
    "    def routing_edge(self, state: ChunkEvalState):\n",
    "        \"\"\"\n",
    "        Determines the next state based on the current chunk index.\n",
    "        If there are more chunks to evaluate, it returns 'chunk_eval', otherwise it returns 'END'.\n",
    "        \"\"\"\n",
    "        if state.chunk_idx >= state.chunks_num - 1:\n",
    "            return 'save_chunks_eval'\n",
    "        return 'context_update'\n",
    "    \n",
    "    def save_chunks_eval_node(self, state: ChunkEvalState):\n",
    "        print(f\"Saving evaluation results for query {state.query_idx} on document {state.doc_file}...\")\n",
    "        eval_file = os.path.join(self.saving_path, f\"{'.'.join(os.path.basename(state.doc_file).split('.')[:-1])}_{state.query_idx}.json\")\n",
    "        with open(eval_file, 'w') as f:\n",
    "            json.dump({\n",
    "                'query': state.query,\n",
    "                'query_idx': state.query_idx,\n",
    "                'doc_file': state.doc_file,\n",
    "                'title': state.title,\n",
    "                'chunks_eval': state.chunks_eval\n",
    "            }, f, indent=4)\n",
    "\n",
    "        return {'chunk_idx': state.chunk_idx + 1}\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        Builds the state graph for chunk evaluation.\n",
    "        \"\"\"\n",
    "        workflow = StateGraph(ChunkEvalState)\n",
    "        workflow.add_node('chunking', self.chunking_node)\n",
    "        workflow.add_node('chunk_eval', self.chunk_eval_node)\n",
    "        workflow.add_node('context_update', self.context_update_node)\n",
    "        workflow.add_node('context_aggregation', self.context_aggregation_node)\n",
    "        workflow.add_node('save_chunks_eval', self.save_chunks_eval_node)\n",
    "\n",
    "        workflow.add_edge(START, 'chunking')\n",
    "        workflow.add_edge('chunking', 'chunk_eval')\n",
    "        workflow.add_conditional_edges('chunk_eval', self.routing_edge, ['context_update', 'save_chunks_eval'])\n",
    "        workflow.add_conditional_edges('context_update', self.context_aggregation_routing, ['context_aggregation', 'chunk_eval'])\n",
    "        workflow.add_edge('context_aggregation', 'chunk_eval')\n",
    "        workflow.add_edge('save_chunks_eval', END)\n",
    "\n",
    "        self.graph = workflow.compile(checkpointer=self.memory)\n",
    "\n",
    "    def run(self, query: str, query_idx: int, doc_file: str, title: str, summary: str, thread_id: str = None):\n",
    "        \"\"\"\n",
    "        Runs the graph to evaluate chunks of a document based on a query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to evaluate chunks against.\n",
    "            query_idx (int): The index of the query in the list of queries.\n",
    "            doc_file (str): The path to the document file to be evaluated.\n",
    "            title (str): The title of the document.\n",
    "            summary (str): The summary of the document.\n",
    "            thread_id (str): Optional thread ID for memory management.\n",
    "        \"\"\"\n",
    "        config = {'configurable': {'thread_id': thread_id}} if thread_id else None\n",
    "        return self.graph.invoke({\n",
    "            'query': query,\n",
    "            'query_idx': query_idx,\n",
    "            'doc_file': doc_file,\n",
    "            'title': title,\n",
    "            'summary': summary\n",
    "        }, config=config)\n",
    "    \n",
    "    def run_with_progress(self, query: str, query_idx: int, doc_file: str, title: str, summary: str, thread_id: str = None):\n",
    "        \"\"\"\n",
    "        Runs the graph with a progress bar to track chunk evaluations.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to evaluate chunks against.\n",
    "            query_idx (int): The index of the query in the list of queries.\n",
    "            doc_file (str): The path to the document file to be evaluated.\n",
    "            title (str): The title of the document.\n",
    "            summary (str): The summary of the document.\n",
    "            thread_id (str): Optional thread ID for memory management.\n",
    "        \"\"\"\n",
    "        config = {'configurable': {'thread_id': thread_id}} if thread_id else None\n",
    "        state = self.graph.invoke({\n",
    "            'query': query,\n",
    "            'query_idx': query_idx,\n",
    "            'doc_file': doc_file,\n",
    "            'title': title,\n",
    "            'summary': summary\n",
    "        }, \n",
    "        config=config,\n",
    "        interrupt_before='chunk_eval')\n",
    "\n",
    "        chunks_num = state['chunks_num']\n",
    "        positive_evaluations = 0\n",
    "        negative_evaluations = 0\n",
    "\n",
    "        with tqdm(total=chunks_num, desc=\"Evaluating Chunks\", postfix={\"Positive\": positive_evaluations, \"Negative\": negative_evaluations, \"Query_id\": query_idx}) as pbar:\n",
    "            while state['chunk_idx'] < chunks_num:\n",
    "                # Invoke the next step in the graph\n",
    "                state = self.graph.invoke(input=None, config={'configurable': {'thread_id': thread_id}}, interrupt_before='chunk_eval')\n",
    "                \n",
    "                # Update evaluations\n",
    "                positive_evaluations = sum([1 for chunk in state['chunks_eval'] if chunk['label']])\n",
    "                negative_evaluations = sum([1 for chunk in state['chunks_eval'] if not chunk['label']])\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(Positive=positive_evaluations, Negative=negative_evaluations)\n",
    "\n",
    "    async def run_with_progress_async(self, query: str, query_idx: int, doc_file: str, title: str, summary: str, thread_id: str = None):\n",
    "        \"\"\"\n",
    "        Runs the graph asynchronously with a progress bar to track chunk evaluations.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query to evaluate chunks against.\n",
    "            query_idx (int): The index of the query in the list of queries.\n",
    "            doc_file (str): The path to the document file to be evaluated.\n",
    "            title (str): The title of the document.\n",
    "            summary (str): The summary of the document.\n",
    "            thread_id (str): Optional thread ID for memory management.\n",
    "\n",
    "        Returns:\n",
    "            dict: The final state after all chunks are evaluated.\n",
    "        \"\"\"\n",
    "        config = {'configurable': {'thread_id': thread_id}} if thread_id else None\n",
    "        state = await asyncio.to_thread(self.graph.invoke, {\n",
    "            'query': query,\n",
    "            'query_idx': query_idx,\n",
    "            'doc_file': doc_file,\n",
    "            'title': title,\n",
    "            'summary': summary\n",
    "        }, config=config, interrupt_before='chunk_eval')\n",
    "\n",
    "        chunks_num = state['chunks_num']\n",
    "        positive_evaluations = 0\n",
    "        negative_evaluations = 0\n",
    "\n",
    "        # Initialize the progress bar\n",
    "        pbar = notebook_tqdm(total=chunks_num, desc=\"Evaluating Chunks\", postfix={\"Positive\": positive_evaluations, \"Negative\": negative_evaluations, \"Query_id\": query_idx})\n",
    "        try:\n",
    "            while state['chunk_idx'] < chunks_num:\n",
    "                # Invoke the next step in the graph asynchronously\n",
    "                state = await asyncio.to_thread(self.graph.invoke, input=None, config={'configurable': {'thread_id': thread_id}}, interrupt_before='chunk_eval')\n",
    "\n",
    "                # Update evaluations\n",
    "                positive_evaluations = sum([1 for chunk in state['chunks_eval'] if chunk['label']])\n",
    "                negative_evaluations = sum([1 for chunk in state['chunks_eval'] if not chunk['label']])\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(Positive=positive_evaluations, Negative=negative_evaluations, Query_id=query_idx)\n",
    "        finally:\n",
    "            pbar.close()\n",
    "\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "32f97b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "async def run_all_queries_async(chunk_eval_graph, metadata_file_path):\n",
    "    \"\"\"\n",
    "    Runs run_with_progress_async concurrently for all queries in the metadata file.\n",
    "\n",
    "    Args:\n",
    "        chunk_eval_graph (ChunkEvalGraph): The ChunkEvalGraph instance to run evaluations.\n",
    "        metadata_file_path (str): Path to the metadata file containing queries and document information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the results for each query.\n",
    "    \"\"\"\n",
    "    # Load metadata file\n",
    "    with open(metadata_file_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    queries = metadata.get('queries', [])\n",
    "    doc_file = metadata.get('path')\n",
    "    title = metadata.get('title')\n",
    "    summary = metadata.get('summary')\n",
    "\n",
    "    if not queries or not doc_file or not title or not summary:\n",
    "        raise ValueError(\"Metadata file is missing required fields.\")\n",
    "\n",
    "    async def run_query_async(query_idx, query):\n",
    "        thread_id = uuid4().hex\n",
    "        inputs = {\n",
    "            'query': query,\n",
    "            'query_idx': query_idx,\n",
    "            'doc_file': doc_file,\n",
    "            'title': title,\n",
    "            'summary': summary\n",
    "        }\n",
    "        #print(f\"Starting async evaluation for query {query_idx}...\")\n",
    "        state = await chunk_eval_graph.run_with_progress_async(**inputs, thread_id=thread_id)\n",
    "        return query_idx, state\n",
    "\n",
    "    # Run all queries concurrently\n",
    "    tasks = [run_query_async(idx, query) for idx, query in enumerate(queries)]\n",
    "    results = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2701cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = get_checkpointer(checkpointer_mode='local', mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5c0f2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_eval_graph = ChunkEvalGraph(\n",
    "    llm=get_llm(llm_model='google'),\n",
    "    prompts_config={\n",
    "        'path': '../prompts',\n",
    "        'chunk_eval_system': 'chunk_eval_system',\n",
    "        'chunk_eval_task': 'chunk_eval_task',\n",
    "        'doc_context_system': 'doc_context_system',\n",
    "        'doc_context_update': 'doc_context_update',\n",
    "        'doc_context_aggregation': 'doc_context_aggregation'\n",
    "    },\n",
    "    memory=memory,\n",
    "    docs_metadata_path='../docs_metadata',\n",
    "    saving_path='../chunk_eval_results',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=0,\n",
    "    context_agg_interval=5,\n",
    "    prompt_manager_spec={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1bd76520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c61d3803aca4998884744a7009a4c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aa6c4bd9d3423fb0602b8cdfc7df4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8199dad7eff943739c0c3ce6dab39545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=2]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69e673fb64c4327818b0bd4336e5cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ef1d71893b49258d63a95bbbf9607a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13b5e0c9d17496687b8cd0ec33e076b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=6]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f23444934944e5ba5ead766bd58c274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=7]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472497b2f42643b1972de14265e4f4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7286cdb836f54c10b2efb4258c8fc36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=8]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7b6f3516c34ef8a5efc1a796d1ec69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=9]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results for query 8 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 1 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 4 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 2 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 0 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 5 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 6 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 7 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 3 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 9 on document ../docs/2502.09625v1.pdf...\n"
     ]
    }
   ],
   "source": [
    "await run_all_queries_async(chunk_eval_graph, \"../docs_metadata/2502.09625v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "b76e9fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How does Stockformer differ from the original Transformer model?',\n",
       " 'query_idx': 1,\n",
       " 'doc_file': '../docs/2502.09625v1.pdf',\n",
       " 'title': 'Transformer Based Time-Series Forecasting for Stock',\n",
       " 'summary': 'To the naked eye, stock prices are considered chaotic, dynamic, and\\nunpredictable. Indeed, it is one of the most difficult forecasting tasks that\\nhundreds of millions of retail traders and professional traders around the\\nworld try to do every second even before the market opens. With recent advances\\nin the development of machine learning and the amount of data the market\\ngenerated over years, applying machine learning techniques such as deep\\nlearning neural networks is unavoidable. In this work, we modeled the task as a\\nmultivariate forecasting problem, instead of a naive autoregression problem.\\nThe multivariate analysis is done using the attention mechanism via applying a\\nmutated version of the Transformer, \"Stockformer\", which we created.'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_metadata_path = '../docs_metadata'\n",
    "\n",
    "metadata_file = '2502.09625v1.json'\n",
    "query_idx = 1\n",
    "\n",
    "with open(os.path.join(docs_metadata_path, metadata_file), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "paper_query = metadata['queries'][query_idx]\n",
    "\n",
    "inputs = {\n",
    "    'query': paper_query,\n",
    "    'query_idx': query_idx,\n",
    "    'doc_file': metadata['path'],\n",
    "    'title': metadata['title'],\n",
    "    'summary': metadata['summary']\n",
    "}\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a9b14a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = 'Stockformer1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9faa4322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking document: ../docs/2502.09625v1.pdf into 59 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Chunks: 100%|| 59/59 [02:26<00:00,  2.49s/it, Negative=48, Positive=11]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results for query 1 on document ../docs/2502.09625v1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state = chunk_eval_graph.run_with_progress(**inputs, thread_id=thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "894598e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = memory.get({'configurable': {'thread_id': thread_id}})['channel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f6736a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive evaluations: 11\n",
      "Negative evaluations: 48\n",
      "\n",
      "CONTEXT:\n",
      "```\n",
      "## Summary of previous sections:\n",
      "- Stockformer is a modified Transformer model for hourly stock price forecasting, utilizing ProbSparse Attention and Self-attention Distilling for efficiency.\n",
      "- Stockformer's performance benefits from larger embedding sizes and more attention heads.\n",
      "- Stockformer with ProbSparse Attention is more efficient than Full Attention and more profitable than LSTM models.\n",
      "- Future research will focus on incorporating more stock tickers, dynamic training methods, and exploring Time2Vec and TimeFrame2Vec temporal encoding to improve the model's profitability and adaptability.\n",
      "- The paper provides a list of references used in the research.\n",
      "\n",
      "## Current Section:\n",
      "The paper provides a list of references used in the research. This section lists academic papers and publications that were cited in the study. The list of references continues from the previous chunk, completing the bibliography.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "positive_evaluations = sum([1 for chunk in state['chunks_eval'] if chunk['label']])\n",
    "negative_evaluations = sum([1 for chunk in state['chunks_eval'] if not chunk['label']])\n",
    "print(f\"Positive evaluations: {positive_evaluations}\\nNegative evaluations: {negative_evaluations}\")\n",
    "\n",
    "print(\"\\nCONTEXT:\")\n",
    "for line in state['context'].split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ae0f8c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1 (Positive):\n",
      "the market generated over years, applying machine learning\n",
      "techniques such as deep learning neural networks is unavoidable.\n",
      "In this work, we modeled the task as a multivariate forecast-\n",
      "ing problem, instead of a naive autoregression problem. The\n",
      "multivariate analysis is done using the attention mechanism via\n",
      "applying a mutated version of the Transformer, Stockformer,\n",
      "which we created.\n",
      "I. I NTRODUCTION\n",
      "Predicting the financial time series such as stock price\n",
      "means predicting the behavior of the stock price steps ahead of\n",
      "the series with the help of various variables. By knowing the\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 14 (Positive):\n",
      "In this project, we implemented Stockformer on the top of\n",
      "the Transformer, discussed issues of naive Transformer, and\n",
      "changed the original architecture to fit with the financial ticker\n",
      "forecasting task.\n",
      "III. P ROBLEM FORMULATION\n",
      "Although the goal for the neural network is to predict the\n",
      "stock price ahead, the task for this project is to assist traders\n",
      "to make a profit in the end. Since we are assisting the human\n",
      "trader and not doing the high-frequency trading, we need to\n",
      "give the human trader enough time to react with the output\n",
      "of the model. Therefore, together with the need to capture\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 24 (Positive):\n",
      "By summing and comparing the p-value on each row, pre-\n",
      "dicting the stock price of W&T Offshore is found to be most\n",
      "beneficial when the stock prices of other companies are in\n",
      "the model. Therefore, we choose to predict the stock price\n",
      "of W&T Offshore and use neural network model to take\n",
      "advantage of the causality among the stock prices.\n",
      "\fV. A RCHITECTURE\n",
      "When it comes to design a neural network based on\n",
      "Transformer architecture, there are many choices such as\n",
      "different ways of doing embeddings, encoders, and decoders.\n",
      "The following section of the report will focus on discussing\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 25 (Positive):\n",
      "the design choices for stockformer.\n",
      "A. Token Embedding Design\n",
      "In most of the use cases, the token embedding layer in a\n",
      "Transformer-based model learns a fixed-length vector repre-\n",
      "sentation of a variable-length sequence input. The Embedding\n",
      "layer will keep the sequence length while extracting more\n",
      "features from the input in each time step. During our devel-\n",
      "opment, we have two options for token embedding design.\n",
      "1) Fully Connected Based: The embedding of the input\n",
      "sequence will be learned via several linear layers. However, the\n",
      "temporal information will be lost during the operation because,\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 26 (Positive):\n",
      "in order to keep the sequence length, the linear layers will\n",
      "be only learning the patterns among the financial securities in\n",
      "each time step and the relations related to time will be ignored.\n",
      "2) 1D-CNN Based: Assuming there will be sin number\n",
      "of financial securities and data of n time steps are known\n",
      "before the prediction. The 1D-CNN will have sin channels\n",
      "as its input and sout channels in its output. Hence, during the\n",
      "1D convolutional operation, there will be separated kernels\n",
      "for each financial securities. With each financial securities, a\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 27 (Positive):\n",
      "kernel window will be sliding through the time steps to learn\n",
      "the temporal information in the sequence. In the end, the out-\n",
      "put channels will store the fine-grained temporal information\n",
      "learned from each financial securities. In addition, to keep\n",
      "sequence length, the 1D CNN layer has a kernel of 3, stride\n",
      "of 1, padding of 1. Meanwhile, with the padding mode set\n",
      "to circular. the edges of the data are stitched together to\n",
      "avoid boundary effects and can improve the accuracy of the\n",
      "convolutional layer.\n",
      "Fig. 4. The sequence length stays as n, but more sequence patterns have\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 28 (Positive):\n",
      "been extracted in each output channel;\n",
      "B. Encoder Design Choices\n",
      "When forecasting the stock price, the task can be modeled\n",
      "as long sequence time-series forecasting (LSTF) question. The\n",
      "challenges for LSTF include capturing the long-range depen-\n",
      "dency and efficient operations on capturing the dependencies\n",
      "on long sequence. We consider two choices when designing\n",
      "the encoder.\n",
      "Fig. 5. Encoder layers stack together;\n",
      "1) Full Attention: Full attention mechanism is applied on\n",
      "the naive Transformer. The length for capturing a dependency\n",
      "on a sequence is theoretically O(1) which avoids the recurrent\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 29 (Positive):\n",
      "structure and outperforms RNN models. However, as shown\n",
      "in Fig 5, when numerous encoder layers stack together and\n",
      "each of the attention layer contains a multi-head attention\n",
      "block, the memory usage becomes a bottleneck. Assuming\n",
      "the sequence length is L, each multi-head attention block will\n",
      "require O(L2) memory space. And if there are J encoder layers\n",
      "stacking together, the memory complexity will be O(J  L2).\n",
      "This creates higher hardware requirement during the training\n",
      "and makes real-time prediction expensive.\n",
      "2) ProbSparse Attention & Self-attention Distilling: Aim-\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 30 (Positive):\n",
      "ing to solve time and memory complexity issues in the naive\n",
      "Transformer. This project considers of using the ProbSparse\n",
      "Attention and Self-attention Distilling techniques from the\n",
      "Informer [19]. When calculating the attention score in each\n",
      "multi-head attention layer, a subset number of keys will be\n",
      "selected and follow the attention score equation below:\n",
      "As shown in the equation, according to [19], the top subset\n",
      "number of attention scores will be subtracted by the average\n",
      "attention score across all the queries and selected subset of\n",
      "keys. This will decrease the time and space complexity to\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 31 (Positive):\n",
      "O(L  log(L)). For Self-attention Distilling, as shown in the\n",
      "equation below:\n",
      "\fAt the end of each encoder layer, a max-pooling layer with\n",
      "stride of 2 is added to down-sample the output by half.\n",
      "According to [19], the total memory usage for the whole\n",
      "encoder structure will be reduced to O((2)Llog(L)) where\n",
      " is a very small number.\n",
      "VI. T RAINING DESIGN\n",
      "When it comes to train the model, different design choices\n",
      "of loss functions and learning rate schedulers affect the per-\n",
      "formance of the model in the real world.\n",
      "A. Loss Function\n",
      "So with stock market prediction the obvious goal is to make\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 46 (Positive):\n",
      "for the patterns among these extracted time series patterns.\n",
      "Therefore, to increase the performance of the model, as the\n",
      "embedding size increases the number of attention heads should\n",
      "also increase.\n",
      "D. Full Attention and ProbSparse Attention comparisons\n",
      "For ProbSparse Attention, when compared against zeros\n",
      "(not executing any trading strategies), the ProbSparse Atten-\n",
      "tion wins because it gains money in both the validation and\n",
      "testing set over the long run. This indicates that the ProbSparse\n",
      "Attention from the Informer generalizes well on predicting the\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in state['chunks_eval']:\n",
    "    if chunk['label']:\n",
    "        print(f\"\\nChunk {chunk['idx']} (Positive):\")\n",
    "        for line in chunk['text'].split('\\n'):\n",
    "            print(line)\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0e1e1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = 'Byzantine7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eac1bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking document: ../docs/2009.08294v1.pdf into 44 chunks.\n"
     ]
    }
   ],
   "source": [
    "state = chunk_eval_graph.graph.invoke(\n",
    "    input=inputs,\n",
    "    config={'configurable': {'thread_id': thread_id}},\n",
    "    interrupt_before='chunk_eval'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d7c4ea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive evaluations: 3\n",
      "Negative evaluations: 7\n",
      "\n",
      "CONTEXT:\n",
      "## Summary of previous sections:\n",
      "- The paper examines robust aggregation methods in federated learning (FL) for healthcare, aiming to protect privacy and defend against malicious clients by detecting and discarding them during training.\n",
      "- Experiments use healthcare datasets to evaluate robust FL aggregation against poisoning attacks, demonstrating that privacy-preserving methods can be combined with Byzantine-robust aggregation without significantly impacting learning.\n",
      "- The paper addresses the challenges of sharing private patient data and introduces federated learning (FL) as a solution, while also acknowledging privacy and robustness concerns like biased datasets and poisoning attacks.\n",
      "- The paper's approach involves using k-anonymity and differential privacy (DP), modeling poisoning attack strategies, and using healthcare datasets to evaluate robust aggregation methods in FL.\n",
      "- The main contributions include evaluating robust aggregation methods within a privacy-preserving FL system for healthcare, surveying state-of-the-art methods, evaluating the impact of data privacy engineering on model accuracy, and providing a public implementation framework.\n",
      "- Standard federated learning (FL) involves clients training models locally and sharing them with a server, which aggregates these models to update a global model.\n",
      "\n",
      "## Current Section:\n",
      "This section continues to describe standard aggregation methods in federated learning, focusing on Stochastic Gradient Descent (SGD) and its variants. It mentions FedSGD, where clients send a fraction of their gradients to the server for averaging, and FedAvg, a more efficient version where clients train on multiple batches locally.\n"
     ]
    }
   ],
   "source": [
    "state = chunk_eval_graph.graph.invoke(\n",
    "    input=None,\n",
    "    config={'configurable': {'thread_id': thread_id}},\n",
    "    interrupt_before='chunk_eval'\n",
    ")\n",
    "\n",
    "positive_evaluations = sum([1 for chunk in state['chunks_eval'] if chunk['label']])\n",
    "negative_evaluations = sum([1 for chunk in state['chunks_eval'] if not chunk['label']])\n",
    "print(f\"Positive evaluations: {positive_evaluations}\\nNegative evaluations: {negative_evaluations}\")\n",
    "\n",
    "print(\"\\nCONTEXT:\")\n",
    "for line in state['context'].split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739dd1b9",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
