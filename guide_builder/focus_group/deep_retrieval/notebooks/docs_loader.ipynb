{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage, AIMessage, AnyMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import List, Dict, Any, TypedDict, Annotated\n",
    "from pydantic import BaseModel\n",
    "from langgraph.constants import Send\n",
    "import json\n",
    "from operator import add\n",
    "from fcgb.cfg.precompiled import get_llm, get_checkpointer\n",
    "from fcgb.prompt_manager import PromptManager\n",
    "from rag_eval import RandomQueriesPaperSearchGraph, ChunkEvalGraph, run_all_queries_async\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8547ff",
   "metadata": {},
   "source": [
    "## download documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_runner = RandomQueriesPaperSearchGraph(\n",
    "    llm=get_llm(llm_model='google'),\n",
    "    prompts_config={\n",
    "        'path': '../prompts',\n",
    "        'random_queries': 'random_queries',\n",
    "        'paper_queries': 'paper_queries'\n",
    "    },\n",
    "    docs_path='../data/docs',\n",
    "    docs_metadata_path='../data/docs_metadata',\n",
    "    main_queries_num=3,\n",
    "    paper_queries_num=10,\n",
    "    max_results=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "238d400f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random queries for paper search...\n",
      "Searching for papers with query: explainable AI methods for fraud detection in banking\n",
      "Searching for papers with query: federated learning privacy preserving techniques healthcare\n",
      "Searching for papers with query: transformer networks time series forecasting stock market\n",
      "Downloading PDF for: MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\n",
      "Downloading PDF for: Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field\n",
      "  and CNNs for Stock Return Predictions\n",
      "Downloading PDF for: An Evaluation of Deep Learning Models for Stock Market Trend Prediction\n",
      "Downloading PDF for: Transformer Based Time-Series Forecasting for Stock\n",
      "Downloading PDF for: Stock Market Telepathy: Graph Neural Networks Predicting the Secret\n",
      "  Conversations between MINT and G7 Countries\n",
      "Downloading PDF for: Vision Through the Veil: Differential Privacy in Federated Learning for\n",
      "  Medical Image Classification\n",
      "Downloading PDF for: Robust Aggregation for Adaptive Privacy Preserving Federated Learning in\n",
      "  Healthcare\n",
      "Downloading PDF for: Privacy Preserving Machine Learning for Electronic Health Records using\n",
      "  Federated Learning and Differential Privacy\n",
      "Downloading PDF for: Complex-valued Federated Learning with Differential Privacy and MRI\n",
      "  Applications\n",
      "Downloading PDF for: Blockchain Integrated Federated Learning in Edge-Fog-Cloud Systems for\n",
      "  IoT based Healthcare Applications A Survey\n",
      "Downloading PDF for: SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative\n",
      "  Mask Learning\n",
      "Downloading PDF for: Computer-Assisted Fraud Detection, From Active Learning to Reward\n",
      "  Maximization\n",
      "Downloading PDF for: A novel approach to increase scalability while training machine learning\n",
      "  algorithms using Bfloat 16 in credit card fraud detection\n",
      "Downloading PDF for: Credit Card Fraud Detection: A Deep Learning Approach\n",
      "Downloading PDF for: Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud\n",
      "  Detection\n",
      "Generating queries for paper: A novel approach to increase scalability while training machine learning\n",
      "  algorithms using Bfloat 16 in credit card fraud detection\n",
      "Generating queries for paper: SEFraud: Graph-based Self-Explainable Fraud Detection via Interpretative\n",
      "  Mask Learning\n",
      "Generating queries for paper: Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud\n",
      "  Detection\n",
      "Generating queries for paper: Vision Through the Veil: Differential Privacy in Federated Learning for\n",
      "  Medical Image Classification\n",
      "Generating queries for paper: Computer-Assisted Fraud Detection, From Active Learning to Reward\n",
      "  Maximization\n",
      "Generating queries for paper: Complex-valued Federated Learning with Differential Privacy and MRI\n",
      "  Applications\n",
      "Generating queries for paper: Privacy Preserving Machine Learning for Electronic Health Records using\n",
      "  Federated Learning and Differential Privacy\n",
      "Generating queries for paper: Credit Card Fraud Detection: A Deep Learning Approach\n",
      "Generating queries for paper: Blockchain Integrated Federated Learning in Edge-Fog-Cloud Systems for\n",
      "  IoT based Healthcare Applications A Survey\n",
      "Generating queries for paper: MASTER: Market-Guided Stock Transformer for Stock Price Forecasting\n",
      "Generating queries for paper: Quantum-Enhanced Forecasting: Leveraging Quantum Gramian Angular Field\n",
      "  and CNNs for Stock Return Predictions\n",
      "Generating queries for paper: Robust Aggregation for Adaptive Privacy Preserving Federated Learning in\n",
      "  Healthcare\n",
      "Generating queries for paper: An Evaluation of Deep Learning Models for Stock Market Trend Prediction\n",
      "Generating queries for paper: Transformer Based Time-Series Forecasting for Stock\n",
      "Generating queries for paper: Stock Market Telepathy: Graph Neural Networks Predicting the Secret\n",
      "  Conversations between MINT and G7 Countries\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'queries': QueriesModel(queries=['explainable AI methods for fraud detection in banking', 'federated learning privacy preserving techniques healthcare', 'transformer networks time series forecasting stock market']),\n",
       " 'metadata_files': ['../docs_metadata/2406.11389v1.json',\n",
       "  '../docs_metadata/1811.08212v1.json',\n",
       "  '../docs_metadata/2206.12415v1.json',\n",
       "  '../docs_metadata/2409.13406v1.json',\n",
       "  '../docs_metadata/2108.02501v3.json',\n",
       "  '../docs_metadata/2306.17794v1.json',\n",
       "  '../docs_metadata/2009.08294v1.json',\n",
       "  '../docs_metadata/2406.15962v1.json',\n",
       "  '../docs_metadata/2110.03478v2.json',\n",
       "  '../docs_metadata/2406.05517v1.json',\n",
       "  '../docs_metadata/2312.15235v1.json',\n",
       "  '../docs_metadata/2310.07427v3.json',\n",
       "  '../docs_metadata/2408.12408v1.json',\n",
       "  '../docs_metadata/2502.09625v1.json',\n",
       "  '../docs_metadata/2506.01945v1.json']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_runner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab99d9",
   "metadata": {},
   "source": [
    "## Chunks labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "2701cd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = get_checkpointer(checkpointer_mode='local', mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_eval_graph = ChunkEvalGraph(\n",
    "    llm=get_llm(llm_model='google'),\n",
    "    prompts_config={\n",
    "        'path': '../prompts',\n",
    "        'chunk_eval_system': 'chunk_eval_system',\n",
    "        'chunk_eval_task': 'chunk_eval_task',\n",
    "        'doc_context_system': 'doc_context_system',\n",
    "        'doc_context_update': 'doc_context_update',\n",
    "        'doc_context_aggregation': 'doc_context_aggregation'\n",
    "    },\n",
    "    memory=memory,\n",
    "    docs_metadata_path='../data/docs_metadata',\n",
    "    saving_path='../data/chunk_eval_results',\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=0,\n",
    "    context_agg_interval=5,\n",
    "    prompt_manager_spec={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1bd76520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c61d3803aca4998884744a7009a4c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=3]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44aa6c4bd9d3423fb0602b8cdfc7df4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=4]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8199dad7eff943739c0c3ce6dab39545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=2]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69e673fb64c4327818b0bd4336e5cdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=5]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ef1d71893b49258d63a95bbbf9607a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13b5e0c9d17496687b8cd0ec33e076b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=6]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f23444934944e5ba5ead766bd58c274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=7]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472497b2f42643b1972de14265e4f4ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=1]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7286cdb836f54c10b2efb4258c8fc36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=8]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7b6f3516c34ef8a5efc1a796d1ec69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating Chunks:   0%|          | 0/59 [00:00<?, ?it/s, Negative=0, Positive=0, Query_id=9]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results for query 8 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 1 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 4 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 2 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 0 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 5 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 6 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 7 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 3 on document ../docs/2502.09625v1.pdf...\n",
      "Saving evaluation results for query 9 on document ../docs/2502.09625v1.pdf...\n"
     ]
    }
   ],
   "source": [
    "await run_all_queries_async(chunk_eval_graph, \"../docs_metadata/2502.09625v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e9fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How does Stockformer differ from the original Transformer model?',\n",
       " 'query_idx': 1,\n",
       " 'doc_file': '../docs/2502.09625v1.pdf',\n",
       " 'title': 'Transformer Based Time-Series Forecasting for Stock',\n",
       " 'summary': 'To the naked eye, stock prices are considered chaotic, dynamic, and\\nunpredictable. Indeed, it is one of the most difficult forecasting tasks that\\nhundreds of millions of retail traders and professional traders around the\\nworld try to do every second even before the market opens. With recent advances\\nin the development of machine learning and the amount of data the market\\ngenerated over years, applying machine learning techniques such as deep\\nlearning neural networks is unavoidable. In this work, we modeled the task as a\\nmultivariate forecasting problem, instead of a naive autoregression problem.\\nThe multivariate analysis is done using the attention mechanism via applying a\\nmutated version of the Transformer, \"Stockformer\", which we created.'}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_metadata_path = '../data/docs_metadata'\n",
    "\n",
    "metadata_file = '2502.09625v1.json'\n",
    "query_idx = 1\n",
    "\n",
    "with open(os.path.join(docs_metadata_path, metadata_file), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "paper_query = metadata['queries'][query_idx]\n",
    "\n",
    "inputs = {\n",
    "    'query': paper_query,\n",
    "    'query_idx': query_idx,\n",
    "    'doc_file': metadata['path'],\n",
    "    'title': metadata['title'],\n",
    "    'summary': metadata['summary']\n",
    "}\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "a9b14a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = 'Stockformer1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9faa4322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking document: ../docs/2502.09625v1.pdf into 59 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Chunks: 100%|██████████| 59/59 [02:26<00:00,  2.49s/it, Negative=48, Positive=11]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving evaluation results for query 1 on document ../docs/2502.09625v1.pdf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "state = chunk_eval_graph.run_with_progress(**inputs, thread_id=thread_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "894598e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = memory.get({'configurable': {'thread_id': thread_id}})['channel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f6736a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive evaluations: 11\n",
      "Negative evaluations: 48\n",
      "\n",
      "CONTEXT:\n",
      "```\n",
      "## Summary of previous sections:\n",
      "- Stockformer is a modified Transformer model for hourly stock price forecasting, utilizing ProbSparse Attention and Self-attention Distilling for efficiency.\n",
      "- Stockformer's performance benefits from larger embedding sizes and more attention heads.\n",
      "- Stockformer with ProbSparse Attention is more efficient than Full Attention and more profitable than LSTM models.\n",
      "- Future research will focus on incorporating more stock tickers, dynamic training methods, and exploring Time2Vec and TimeFrame2Vec temporal encoding to improve the model's profitability and adaptability.\n",
      "- The paper provides a list of references used in the research.\n",
      "\n",
      "## Current Section:\n",
      "The paper provides a list of references used in the research. This section lists academic papers and publications that were cited in the study. The list of references continues from the previous chunk, completing the bibliography.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "positive_evaluations = sum([1 for chunk in state['chunks_eval'] if chunk['label']])\n",
    "negative_evaluations = sum([1 for chunk in state['chunks_eval'] if not chunk['label']])\n",
    "print(f\"Positive evaluations: {positive_evaluations}\\nNegative evaluations: {negative_evaluations}\")\n",
    "\n",
    "print(\"\\nCONTEXT:\")\n",
    "for line in state['context'].split('\\n'):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "ae0f8c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1 (Positive):\n",
      "the market generated over years, applying machine learning\n",
      "techniques such as deep learning neural networks is unavoidable.\n",
      "In this work, we modeled the task as a multivariate forecast-\n",
      "ing problem, instead of a naive autoregression problem. The\n",
      "multivariate analysis is done using the attention mechanism via\n",
      "applying a mutated version of the Transformer, ”Stockformer”,\n",
      "which we created.\n",
      "I. I NTRODUCTION\n",
      "Predicting the financial time series such as stock price\n",
      "means predicting the behavior of the stock price steps ahead of\n",
      "the series with the help of various variables. By knowing the\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 14 (Positive):\n",
      "In this project, we implemented Stockformer on the top of\n",
      "the Transformer, discussed issues of naive Transformer, and\n",
      "changed the original architecture to fit with the financial ticker\n",
      "forecasting task.\n",
      "III. P ROBLEM FORMULATION\n",
      "Although the goal for the neural network is to predict the\n",
      "stock price ahead, the task for this project is to assist traders\n",
      "to make a profit in the end. Since we are assisting the human\n",
      "trader and not doing the high-frequency trading, we need to\n",
      "give the human trader enough time to react with the output\n",
      "of the model. Therefore, together with the need to capture\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 24 (Positive):\n",
      "By summing and comparing the p-value on each row, pre-\n",
      "dicting the stock price of W&T Offshore is found to be most\n",
      "beneficial when the stock prices of other companies are in\n",
      "the model. Therefore, we choose to predict the stock price\n",
      "of W&T Offshore and use neural network model to take\n",
      "advantage of the causality among the stock prices.\n",
      "\fV. A RCHITECTURE\n",
      "When it comes to design a neural network based on\n",
      "Transformer architecture, there are many choices such as\n",
      "different ways of doing embeddings, encoders, and decoders.\n",
      "The following section of the report will focus on discussing\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 25 (Positive):\n",
      "the design choices for stockformer.\n",
      "A. Token Embedding Design\n",
      "In most of the use cases, the token embedding layer in a\n",
      "Transformer-based model learns a fixed-length vector repre-\n",
      "sentation of a variable-length sequence input. The Embedding\n",
      "layer will keep the sequence length while extracting more\n",
      "features from the input in each time step. During our devel-\n",
      "opment, we have two options for token embedding design.\n",
      "1) Fully Connected Based: The embedding of the input\n",
      "sequence will be learned via several linear layers. However, the\n",
      "temporal information will be lost during the operation because,\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 26 (Positive):\n",
      "in order to keep the sequence length, the linear layers will\n",
      "be only learning the patterns among the financial securities in\n",
      "each time step and the relations related to time will be ignored.\n",
      "2) 1D-CNN Based: Assuming there will be sin number\n",
      "of financial securities and data of n time steps are known\n",
      "before the prediction. The 1D-CNN will have sin channels\n",
      "as its input and sout channels in its output. Hence, during the\n",
      "1D convolutional operation, there will be separated kernels\n",
      "for each financial securities. With each financial securities, a\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 27 (Positive):\n",
      "kernel window will be sliding through the time steps to learn\n",
      "the temporal information in the sequence. In the end, the out-\n",
      "put channels will store the fine-grained temporal information\n",
      "learned from each financial securities. In addition, to keep\n",
      "sequence length, the 1D CNN layer has a kernel of 3, stride\n",
      "of 1, padding of 1. Meanwhile, with the padding mode set\n",
      "to circular. the edges of the data are ”stitched” together to\n",
      "avoid boundary effects and can improve the accuracy of the\n",
      "convolutional layer.\n",
      "Fig. 4. The sequence length stays as n, but more sequence patterns have\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 28 (Positive):\n",
      "been extracted in each output channel;\n",
      "B. Encoder Design Choices\n",
      "When forecasting the stock price, the task can be modeled\n",
      "as long sequence time-series forecasting (LSTF) question. The\n",
      "challenges for LSTF include capturing the long-range depen-\n",
      "dency and efficient operations on capturing the dependencies\n",
      "on long sequence. We consider two choices when designing\n",
      "the encoder.\n",
      "Fig. 5. Encoder layers stack together;\n",
      "1) Full Attention: Full attention mechanism is applied on\n",
      "the naive Transformer. The length for capturing a dependency\n",
      "on a sequence is theoretically O(1) which avoids the recurrent\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 29 (Positive):\n",
      "structure and outperforms RNN models. However, as shown\n",
      "in Fig 5, when numerous encoder layers stack together and\n",
      "each of the attention layer contains a multi-head attention\n",
      "block, the memory usage becomes a bottleneck. Assuming\n",
      "the sequence length is L, each multi-head attention block will\n",
      "require O(L2) memory space. And if there are J encoder layers\n",
      "stacking together, the memory complexity will be O(J ∗ L2).\n",
      "This creates higher hardware requirement during the training\n",
      "and makes real-time prediction expensive.\n",
      "2) ProbSparse Attention & Self-attention Distilling: Aim-\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 30 (Positive):\n",
      "ing to solve time and memory complexity issues in the naive\n",
      "Transformer. This project considers of using the ProbSparse\n",
      "Attention and Self-attention Distilling techniques from the\n",
      "Informer [19]. When calculating the attention score in each\n",
      "multi-head attention layer, a subset number of keys will be\n",
      "selected and follow the attention score equation below:\n",
      "As shown in the equation, according to [19], the top subset\n",
      "number of attention scores will be subtracted by the average\n",
      "attention score across all the queries and selected subset of\n",
      "keys. This will decrease the time and space complexity to\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 31 (Positive):\n",
      "O(L ∗ log(L)). For Self-attention Distilling, as shown in the\n",
      "equation below:\n",
      "\fAt the end of each encoder layer, a max-pooling layer with\n",
      "stride of 2 is added to down-sample the output by half.\n",
      "According to [19], the total memory usage for the whole\n",
      "encoder structure will be reduced to O((2−ε)Llog(L)) where\n",
      "ε is a very small number.\n",
      "VI. T RAINING DESIGN\n",
      "When it comes to train the model, different design choices\n",
      "of loss functions and learning rate schedulers affect the per-\n",
      "formance of the model in the real world.\n",
      "A. Loss Function\n",
      "So with stock market prediction the obvious goal is to make\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "Chunk 46 (Positive):\n",
      "for the patterns among these extracted time series patterns.\n",
      "Therefore, to increase the performance of the model, as the\n",
      "embedding size increases the number of attention heads should\n",
      "also increase.\n",
      "D. Full Attention and ProbSparse Attention comparisons\n",
      "For ProbSparse Attention, when compared against zeros\n",
      "(not executing any trading strategies), the ProbSparse Atten-\n",
      "tion wins because it gains money in both the validation and\n",
      "testing set over the long run. This indicates that the ProbSparse\n",
      "Attention from the Informer generalizes well on predicting the\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in state['chunks_eval']:\n",
    "    if chunk['label']:\n",
    "        print(f\"\\nChunk {chunk['idx']} (Positive):\")\n",
    "        for line in chunk['text'].split('\\n'):\n",
    "            print(line)\n",
    "        print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0e1e1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id = 'Byzantine7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "eac1bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking document: ../docs/2009.08294v1.pdf into 44 chunks.\n"
     ]
    }
   ],
   "source": [
    "state = chunk_eval_graph.graph.invoke(\n",
    "    input=inputs,\n",
    "    config={'configurable': {'thread_id': thread_id}},\n",
    "    interrupt_before='chunk_eval'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d7c4ea7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive evaluations: 3\n",
      "Negative evaluations: 7\n",
      "\n",
      "CONTEXT:\n",
      "## Summary of previous sections:\n",
      "- The paper examines robust aggregation methods in federated learning (FL) for healthcare, aiming to protect privacy and defend against malicious clients by detecting and discarding them during training.\n",
      "- Experiments use healthcare datasets to evaluate robust FL aggregation against poisoning attacks, demonstrating that privacy-preserving methods can be combined with Byzantine-robust aggregation without significantly impacting learning.\n",
      "- The paper addresses the challenges of sharing private patient data and introduces federated learning (FL) as a solution, while also acknowledging privacy and robustness concerns like biased datasets and poisoning attacks.\n",
      "- The paper's approach involves using k-anonymity and differential privacy (DP), modeling poisoning attack strategies, and using healthcare datasets to evaluate robust aggregation methods in FL.\n",
      "- The main contributions include evaluating robust aggregation methods within a privacy-preserving FL system for healthcare, surveying state-of-the-art methods, evaluating the impact of data privacy engineering on model accuracy, and providing a public implementation framework.\n",
      "- Standard federated learning (FL) involves clients training models locally and sharing them with a server, which aggregates these models to update a global model.\n",
      "\n",
      "## Current Section:\n",
      "This section continues to describe standard aggregation methods in federated learning, focusing on Stochastic Gradient Descent (SGD) and its variants. It mentions FedSGD, where clients send a fraction of their gradients to the server for averaging, and FedAvg, a more efficient version where clients train on multiple batches locally.\n"
     ]
    }
   ],
   "source": [
    "state = chunk_eval_graph.graph.invoke(\n",
    "    input=None,\n",
    "    config={'configurable': {'thread_id': thread_id}},\n",
    "    interrupt_before='chunk_eval'\n",
    ")\n",
    "\n",
    "positive_evaluations = sum([1 for chunk in state['chunks_eval'] if chunk['label']])\n",
    "negative_evaluations = sum([1 for chunk in state['chunks_eval'] if not chunk['label']])\n",
    "print(f\"Positive evaluations: {positive_evaluations}\\nNegative evaluations: {negative_evaluations}\")\n",
    "\n",
    "print(\"\\nCONTEXT:\")\n",
    "for line in state['context'].split('\\n'):\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
